import re
import os
import json
from typing import Any, Dict, List, Optional, Tuple
from dotenv import load_dotenv
from google import genai
from app.services.file_parser import FileParser

class InterviewEngine:
    def __init__(self):
        load_dotenv()
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            print("âš ï¸ GOOGLE_API_KEY not found. Gemini features will be disabled.")
            self.client = None
        else:
            try:
                # Initialize Client with explicit API key
                self.client = genai.Client(api_key=api_key)
                print("âœ… Gemini Integration Initialized (Model: gemini-2.0-flash)")
            except Exception as e:
                print(f"âš ï¸ Gemini Connection Error: {e}")
                import traceback
                traceback.print_exc()
                self.client = None

        # State Management
        self.chat_history: List[Dict[str, Any]] = []
        self.context: Dict[str, Any] = {}
        
        # Phase Management (INTRO -> MAIN -> CLOSING)
        self.current_phase: str = "INTRO"  # INTRO, MAIN, CLOSING
        self.current_topic_probe_count: int = 0
        self.max_probes_per_topic: int = 2  # ê°™ì€ ì£¼ì œì— ëŒ€í•´ ìµœëŒ€ 2ë²ˆê¹Œì§€ë§Œ ì¶”ê°€ ì§ˆë¬¸
        self.topics_covered: List[str] = []  # ë‹¤ë£¬ ì£¼ì œë“¤ (í”„ë¡œì íŠ¸ëª… ë“±)

    def _call_llm(self, prompt: str) -> str:
        if not self.client:
            return ""
        try:
            response = self.client.models.generate_content(
                model='gemini-2.0-flash',
                contents=prompt
            )
            return response.text.strip()
        except Exception as e:
            print(f"âš ï¸ Gemini Generation Error: {e}")
            return ""

    def _parse_json_response(self, text: str) -> Dict[str, Any]:
        """Extracts and parses JSON from LLM response, handling markdown code blocks."""
        try:
            json_str = text
            if "```json" in text:
                json_str = text.split("```json")[1].split("```")[0].strip()
            elif "```" in text:
                json_str = text.split("```")[1].split("```")[0].strip()
            return json.loads(json_str)
        except json.JSONDecodeError:
            print(f"âš ï¸ Failed to parse JSON from LLM: {text[:100]}...")
            return {}

    def normalize_role(self, target_role: Optional[str]) -> str:
        return target_role or "backend"

    def _resume_summary_for_prompt(self, resume_content: Optional[Dict[str, Any]]) -> str:
        """êµ¬ì¡°í™” í•„ë“œê°€ ë¹„ì–´ ìˆìœ¼ë©´ raw_textë¥¼ ìš°ì„  ì‚¬ìš©í•´ í”„ë¡¬í”„íŠ¸ìš© ìš”ì•½ ë¬¸ìì—´ ë°˜í™˜."""
        if not resume_content:
            return "No resume content provided."
        raw_text = resume_content.get("raw_text") or ""
        use_raw_primary = resume_content.get("_raw_text_primary") or False
        sk = resume_content.get("skills")
        skills_nonempty = (
            (isinstance(sk, list) and len(sk or []) > 0)
            or (isinstance(sk, dict) and (len((sk or {}).get("essential") or []) > 0 or len((sk or {}).get("additional") or []) > 0))
        )
        has_structure = (
            skills_nonempty
            or (isinstance(resume_content.get("education"), list) and len(resume_content.get("education") or []) > 0)
            or (isinstance(resume_content.get("professional_experience"), list) and len(resume_content.get("professional_experience") or []) > 0)
            or (isinstance(resume_content.get("project_experience"), list) and len(resume_content.get("project_experience") or []) > 0)
        )
        if (use_raw_primary or not has_structure) and raw_text:
            return f"Resume (raw text):\n\"\"\"\n{raw_text.strip()}\n\"\"\""
        return json.dumps(resume_content, ensure_ascii=False, indent=2)

    def build_seed_question(self, role: str, resume_content: Optional[Dict[str, Any]], portfolio: Optional[Dict[str, Any]], portfolio_text: Optional[str] = None, difficulty: str = "JUNIOR", previous_questions: List[str] = []) -> Tuple[str, str, List[str]]:
        # Use LLM to generate a contextual seed question (raw_text fallback when structure is empty)
        resume_summary = self._resume_summary_for_prompt(resume_content)
        
        # Difficulty Adjustment
        difficulty_instruction = ""
        if difficulty == "SENIOR":
            difficulty_instruction = "Assess ARCHITECTURE design, scalability, trade-offs, and leadership skills. Ask complex, high-level technical questions."
        else:
            difficulty_instruction = "Assess FUNDAMENTALS, potential, and problem-solving basics. Ask approachable but technically valid questions."
            
        previous_context = ""
        if previous_questions:
            previous_context = f"AVOID repeating these previously asked questions:\n" + "\n".join([f"- {q}" for q in previous_questions])

        prompt = f"""
        You are a technical interviewer for a {role} position.
        Difficulty Level: {difficulty}
        Instruction: {difficulty_instruction}
        
        This is a follow-up question during the interview. Build upon prior context if available.
        
        Resume Summary:
        {resume_summary}

        Portfolio Summary:
        {json.dumps(portfolio, ensure_ascii=False, indent=2)}
        
        Portfolio Parsed Content (PDF/Docx):
        \"\"\"{portfolio_text or "No attached portfolio files."}\"\"\"

        Constraints:
        {previous_context}

        Task:
        Generate an interview question in Korean that explores ONE of these areas (VARY your choice each time):
        1. **Project Experience**: A specific project from their resume - technical challenges, solutions, outcomes
        2. **Professional/Career Experience**: Their role at a company, team collaboration, leadership, or organizational contributions
        3. **Technical Skills**: Deep-dive into a specific technology or skill they claim expertise in
        
        Requirements:
        - Reference a SPECIFIC item from the candidate's resume (project name, company name, or skill)
        - Ask them to explain with specific situation, their role, actions taken, and outcomes
        - **CRITICAL: NEVER use the words "STAR", "STARR", "ìŠ¤íƒ€", or "STAR ë°©ì‹" in your question. Just ask naturally in Korean without mentioning any methodology names.**
        - Matches the {difficulty} level complexity
        - For SENIOR: Focus on architecture decisions, leadership, and strategic impact
        - For JUNIOR: Focus on learning experience, problem-solving approach, and growth
        - DO NOT ask the same question twice.
        
        Example formats:
        - Project: "ì´ë ¥ì„œì— [í”„ë¡œì íŠ¸ëª…] í”„ë¡œì íŠ¸ê°€ ìˆëŠ”ë°, ì´ í”„ë¡œì íŠ¸ì—ì„œ ë§¡ìœ¼ì‹  ì—­í• ê³¼ ì–´ë–¤ ê¸°ìˆ ì  ë„ì „ì´ ìˆì—ˆëŠ”ì§€, ê·¸ë¦¬ê³  ê²°ê³¼ëŠ” ì–´ë• ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”."
        - Career: "[íšŒì‚¬ëª…]ì—ì„œ [ì§ì±…]ìœ¼ë¡œ ê·¼ë¬´í•˜ì‹œë©´ì„œ ê°€ì¥ í° ì„±ê³¼ë¥¼ ë‚¸ ê²½í—˜ì´ ìˆìœ¼ì‹œë‹¤ë©´, ë‹¹ì‹œ ìƒí™©ê³¼ ë³¸ì¸ì´ ê¸°ì—¬í•œ ë¶€ë¶„ì„ ìƒì„¸íˆ ì„¤ëª…í•´ ì£¼ì‹œê² ì–´ìš”?"
        - Skill: "ì´ë ¥ì„œì— [ê¸°ìˆ ëª…]ì— ëŠ¥ìˆ™í•˜ë‹¤ê³  í•˜ì…¨ëŠ”ë°, ì‹¤ì œë¡œ ì–´ë–¤ ìƒí™©ì—ì„œ ì´ ê¸°ìˆ ì„ í™œìš©í•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ì…¨ë‚˜ìš”?"
        
        Output JSON:
        {{
            "question": "The interview question string in Korean",
            "probe_goal": "Short description of what you want to verify",
            "requested_evidence": ["impact metrics", "specific tech stack decision"]
        }}
        """
        
        response_text = self._call_llm(prompt)
        data = self._parse_json_response(response_text)
        
        return (
            data.get("question", "ëŒ€í‘œì ì¸ í”„ë¡œì íŠ¸ ê²½í—˜ì—ì„œ ë§¡ìœ¼ì‹  ì—­í• ê³¼ ì–´ë–»ê²Œ ë¬¸ì œë¥¼ í•´ê²°í•˜ì…¨ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”."),
            data.get("probe_goal", "í•µì‹¬ ì—­ëŸ‰ í™•ì¸"),
            data.get("requested_evidence", ["êµ¬ì²´ì  í–‰ë™", "ì •ëŸ‰ì  ì„±ê³¼"])
        )

    def build_intro_question(self, role: str, resume_content: Optional[Dict[str, Any]], difficulty: str = "JUNIOR") -> Tuple[str, str, List[str]]:
        """Generate an introduction question (self-introduction, motivation)."""
        resume_summary = self._resume_summary_for_prompt(resume_content)
        
        tone_instruction = "Expected Tone: Encouraging and patient." if difficulty == "JUNIOR" else "Expected Tone: Professional and direct."
        
        prompt = f"""
        You are a friendly technical interviewer for a {role} position.
        Difficulty: {difficulty}. {tone_instruction}
        
        This is the VERY FIRST question - an ice-breaker to start the interview.
        
        Resume Summary:
        {resume_summary}

        Task:
        Generate an opening question in Korean that:
        1. Asks the candidate to briefly introduce themselves
        2. Asks about their motivation for applying to this {role} position
        3. Is warm and welcoming to reduce nervousness
        
        Example: "ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ìê¸°ì†Œê°œì™€ í•¨ê»˜, {role} í¬ì§€ì…˜ì— ì§€ì›í•˜ê²Œ ëœ ë™ê¸°ë¥¼ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
        
        Output JSON:
        {{
            "question": "The introduction question in Korean",
            "probe_goal": "ì§€ì› ë™ê¸° ë° ì—´ì • í™•ì¸",
            "requested_evidence": ["career motivation", "role fit"]
        }}
        """
        
        response_text = self._call_llm(prompt)
        data = self._parse_json_response(response_text)
        
        return (
            data.get("question", f"ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ìê¸°ì†Œê°œì™€ í•¨ê»˜, {role} í¬ì§€ì…˜ì— ì§€ì›í•˜ê²Œ ëœ ë™ê¸°ë¥¼ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"),
            data.get("probe_goal", "ì§€ì› ë™ê¸° ë° ì—´ì • í™•ì¸"),
            data.get("requested_evidence", ["career motivation", "role fit"])
        )

    def build_closing_question(self, role: str, difficulty: str = "JUNIOR") -> Tuple[str, str, List[str]]:
        """Generate a closing question based on difficulty."""
        
        if difficulty == "SENIOR":
            closing_question = (
                "ë§ˆì§€ë§‰ ì§ˆë¬¸ì…ë‹ˆë‹¤. ë§Œì•½ ìš°ë¦¬ íšŒì‚¬ì˜ ê¸°ìˆ  ë¦¬ë”ë¡œì„œ í•©ë¥˜í•˜ì‹œê²Œ ëœë‹¤ë©´, "
                "ê°€ì¥ ë¨¼ì € í•´ê²°í•˜ê³  ì‹¶ì€ ê¸°ìˆ ì  ê³¼ì œë‚˜ ë„ì…í•˜ê³  ì‹¶ì€ ë¬¸í™”ê°€ ìˆìœ¼ì‹ ê°€ìš”? "
                
            )
            return (
                closing_question,
                "ë¦¬ë”ì‹­ ë° ê¸°ìˆ  ë¹„ì „ í™•ì¸",
                ["technical vision", "leadership", "strategic thinking"]
            )
        else:
            # JUNIOR/Default
            closing_question = (
                "ë§ˆì§€ë§‰ ì§ˆë¬¸ì…ë‹ˆë‹¤. ìš°ë¦¬ íšŒì‚¬ì— ì…ì‚¬í•˜ì‹œê²Œ ëœë‹¤ë©´, "
                "ì•ìœ¼ë¡œ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì„±ì¥í•˜ê³  ê¸°ì—¬í•˜ê³  ì‹¶ìœ¼ì‹ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”., "
                
            )
            return (
                closing_question,
                "ì„±ì¥ ë¹„ì „ ë° íšŒì‚¬ ì í•©ì„± í™•ì¸",
                ["growth mindset", "company fit", "curiosity"]
            )

    def analyze_answer(self, answer: str) -> Dict[str, Any]:
        """
        STARR ê¸°ë°˜ ë‹µë³€ ë¶„ì„ (ë¬¸ë§¥ ê¸°ë°˜ - í™•ì¥ í”„ë¡¬í”„íŠ¸)

        ì¤‘ìš”: ì§ì ‘ì ì¸ í‚¤ì›Œë“œ("ìƒí™©ì€", "ê²°ê³¼ëŠ”")ê°€ ì—†ì–´ë„
        ë¬¸ë§¥ìƒ í•´ë‹¹ ìš”ì†Œê°€ ì„¤ëª…ë˜ì—ˆë‹¤ë©´ ì¸ì •í•´ì•¼ í•¨
        """
        prompt = f"""
        ë‹¹ì‹ ì€ ë©´ì ‘ ë‹µë³€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ë‹µë³€ì„ STARR ë°©ë²•ë¡ ì— ë”°ë¼ **ê´€ëŒ€í•˜ê²Œ** ë¶„ì„í•˜ì„¸ìš”.

        ## ì§€ì›ì ë‹µë³€:
        "{answer}"

        ## âš ï¸ í•µì‹¬ ì›ì¹™: ë¬¸ë§¥ ê¸°ë°˜ ì¸ì‹ (ë§¤ìš° ì¤‘ìš”!)

        ê° STARR ìš”ì†ŒëŠ” **ì§ì ‘ì ì¸ í‚¤ì›Œë“œ("ìƒí™©ì€", "ê³¼ì œëŠ”", "ê²°ê³¼ëŠ”") ì—†ì´ë„**
        ë¬¸ë§¥ìƒ í•´ë‹¹ ë‚´ìš©ì´ ì•”ì‹œë˜ê±°ë‚˜ í¬í•¨ë˜ë©´ **trueë¡œ ì¸ì •**í•©ë‹ˆë‹¤.
        ë©´ì ‘ìëŠ” í•™ìˆ ì  ìš©ì–´ë¥¼ ì“°ì§€ ì•Šê³  ìì—°ìŠ¤ëŸ½ê²Œ ë§í•©ë‹ˆë‹¤.

        ---

        ## 1. Situation (ìƒí™©) - ì¸ì • ê¸°ì¤€

        ë‹¤ìŒ ì¤‘ **í•˜ë‚˜ë¼ë„** í•´ë‹¹í•˜ë©´ situation = true:

        **ì‹œê°„/ì‹œê¸° í‘œí˜„:**
        - "ë‹¹ì‹œ", "ê·¸ë•Œ", "ë•Œ", "ê¸°ê°„", "ë™ì•ˆ", "ì¤‘ì—", "ë¬´ë µ", "ì‹œì ˆ"
        - "ì‘ë…„", "ì˜¬í•´", "ì§€ë‚œ", "ìµœê·¼", "ì˜ˆì „", "ê³¼ê±°"
        - "í•™ê¸°", "í•™ë…„", "ê°œì›”", "ì£¼ê°„"

        **ì¥ì†Œ/ì¡°ì§ í‘œí˜„:**
        - "í”„ë¡œì íŠ¸", "íšŒì‚¬", "íŒ€", "ë¶€ì„œ", "ì¡°ì§", "ê·¸ë£¹"
        - "í•™êµ", "ëŒ€í•™", "ëŒ€í•™êµ", "ë¶€íŠ¸ìº í”„", "ìº í”„"
        - "ì¸í„´", "ì¸í„´ì‹­", "í˜„ì¥ì‹¤ìŠµ", "ì‹¤ìŠµ"
        - "ìŠ¤íƒ€íŠ¸ì—…", "ê¸°ì—…", "ì—°êµ¬ì†Œ", "ì—°êµ¬ì‹¤", "ë™ì•„ë¦¬"

        **ë§¥ë½ í‘œí˜„:**
        - "ì—ì„œ", "ì—ì„œëŠ”", "ì—ì„œì˜", "ì¤‘ì—", "ë•Œ"
        - "ì²˜ìŒ", "ì‹œì‘", "ì´ˆë°˜", "ì…ì‚¬", "í•©ë¥˜"
        - "ìƒí™©", "ë°°ê²½", "ê³„ê¸°", "ë§¥ë½", "í™˜ê²½"

        **ì˜ˆì‹œ:** "ì¡¸ì—… í”„ë¡œì íŠ¸ì—ì„œ", "ì¸í„´ ë•Œ", "íŒ€ì—ì„œ", "ì‘ë…„ì—" â†’ situation = true

        ---

        ## 2. Task (ê³¼ì œ) - ì¸ì • ê¸°ì¤€

        ë‹¤ìŒ ì¤‘ **í•˜ë‚˜ë¼ë„** í•´ë‹¹í•˜ë©´ task = true:

        **ì—­í• /ë‹´ë‹¹ í‘œí˜„:**
        - "ë‹´ë‹¹", "ë§¡", "ì—­í• ", "ì±…ì„", "ì„ë¬´", "ì—…ë¬´"
        - "ë¦¬ë“œ", "ë¦¬ë”", "PM", "PL", "TL"
        - "í”„ë¡ íŠ¸", "ë°±ì—”ë“œ", "í’€ìŠ¤íƒ"

        **ëª©í‘œ/ê³¼ì œ í‘œí˜„:**
        - "ëª©í‘œ", "ê³¼ì œ", "ë¯¸ì…˜", "ëª©ì "
        - "ìš”êµ¬ì‚¬í•­", "ìš”ì²­", "ë‹ˆì¦ˆ"
        - "í•´ì•¼", "í•„ìš”", "í•˜ê²Œ ë˜", "ë§¡ê²Œ"

        **ë¬¸ì œ/ì´ìŠˆ í‘œí˜„:**
        - "ë¬¸ì œ", "ì´ìŠˆ", "ë²„ê·¸", "ì˜¤ë¥˜", "ì—ëŸ¬", "ì¥ì• "
        - "ê°œì„ ", "ìˆ˜ì •", "ìµœì í™”", "ì„±ëŠ¥"

        **ê¸°ëŠ¥/ëŒ€ìƒ í‘œí˜„:**
        - "ê¸°ëŠ¥", "ëª¨ë“ˆ", "ì»´í¬ë„ŒíŠ¸", "í˜ì´ì§€", "í™”ë©´"
        - "API", "ì„œë¹„ìŠ¤", "ì‹œìŠ¤í…œ", "ì•±", "ì›¹"

        **ì˜ˆì‹œ:** "ë¡œê·¸ì¸ ê¸°ëŠ¥ì„ ë§Œë“¤ì–´ì•¼", "ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”", "ë°±ì—”ë“œ ë‹´ë‹¹" â†’ task = true

        ---

        ## 3. Action (í–‰ë™) - ì¸ì • ê¸°ì¤€

        ë‹¤ìŒ ì¤‘ **í•˜ë‚˜ë¼ë„** í•´ë‹¹í•˜ë©´ action = true:

        **í–‰ë™ ë™ì‚¬ (ê³¼ê±°í˜•):**
        - "í–ˆìŠµë‹ˆë‹¤", "í–ˆì–´ìš”", "í–ˆê³ ", "í•˜ì—¬", "í•´ì„œ"
        - "êµ¬í˜„", "ê°œë°œ", "ì‘ì„±", "ì„¤ê³„", "ì ìš©", "ì‚¬ìš©"
        - "ë°°í¬", "í…ŒìŠ¤íŠ¸", "ìˆ˜ì •", "ì¶”ê°€", "ì—°ë™"

        **ê¸°ìˆ  ìŠ¤íƒ ì–¸ê¸‰:**
        - React, Vue, Angular, Next.js, Spring, Django, FastAPI
        - Java, Python, JavaScript, TypeScript, Kotlin
        - MySQL, PostgreSQL, MongoDB, Redis
        - AWS, Docker, Kubernetes, Git
        - (ê¸°íƒ€ ëª¨ë“  ê¸°ìˆ ëª…)

        **ì‘ì—… ì„¤ëª…:**
        - "ì½”ë“œ", "API", "DB", "ì¿¼ë¦¬", "ë¡œì§"
        - "í˜‘ì—…", "ì†Œí†µ", "íšŒì˜", "ë¦¬ë·°"

        **ì˜ˆì‹œ:** "Reactë¡œ êµ¬í˜„", "Spring ì‚¬ìš©", "API ê°œë°œ", "í…ŒìŠ¤íŠ¸ ì‘ì„±" â†’ action = true

        ---

        ## 4. Result (ê²°ê³¼) - ì¸ì • ê¸°ì¤€

        ë‹¤ìŒ ì¤‘ **í•˜ë‚˜ë¼ë„** í•´ë‹¹í•˜ë©´ result = true:

        **ì™„ë£Œ/ì„±ê³µ í‘œí˜„:**
        - "ì™„ë£Œ", "ì™„ì„±", "ë§ˆë¬´ë¦¬", "ì„±ê³µ"
        - "ì¶œì‹œ", "ì˜¤í”ˆ", "ë°°í¬", "ë¦´ë¦¬ì¦ˆ"
        - "í•´ê²°", "ì²˜ë¦¬", "ë‹¬ì„±"

        **ì„±ê³¼/ê°œì„  í‘œí˜„:**
        - "ê²°ê³¼", "ì„±ê³¼", "íš¨ê³¼", "ì˜í–¥"
        - "ê°œì„ ", "í–¥ìƒ", "ì¦ê°€", "ê°ì†Œ"
        - "ë¹¨ë¼", "ì¢‹ì•„", "ë‚˜ì•„"

        **ìˆ˜ì¹˜/ì§€í‘œ:**
        - í¼ì„¼íŠ¸(%), ë°°ìˆ˜(ë°°), ì¸ì›(ëª…), ê±´ìˆ˜(ê±´)
        - "TPS", "ì‘ë‹µì‹œê°„", "íŠ¸ë˜í”½"

        **ì˜ˆì‹œ:** "ë°°í¬ ì™„ë£Œ", "30% í–¥ìƒ", "ì„±ëŠ¥ ê°œì„ ", "ì¶œì‹œ ì„±ê³µ" â†’ result = true

        ---

        ## 5. Reflection (ì„±ì°°) - ì¸ì • ê¸°ì¤€

        ë‹¤ìŒ ì¤‘ **í•˜ë‚˜ë¼ë„** í•´ë‹¹í•˜ë©´ reflection = true:

        **ë°°ì›€/ê¹¨ë‹¬ìŒ:**
        - "ë°°ì› ", "ë°°ìš´", "ëŠê¼ˆ", "ëŠë‚€", "ê¹¨ë‹¬"
        - "ì•Œê²Œ", "ì´í•´í•˜ê²Œ", "ê²½í—˜"

        **ì„±ì¥/ë°œì „:**
        - "ì„±ì¥", "ë°œì „", "í–¥ìƒ", "ì—­ëŸ‰", "ëŠ¥ë ¥"

        **ë°˜ì„±/ë¶€ì¡±:**
        - "ë¶€ì¡±", "ì•„ì‰¬", "ì–´ë ¤", "í˜ë“¤"

        **ë‹¤ì§/ê³„íš:**
        - "ë‹¤ìŒì—ëŠ”", "ì•ìœ¼ë¡œ", "ë…¸ë ¥", "ê³„íš"

        **ì˜ˆì‹œ:** "ì´ ê²½í—˜ì„ í†µí•´ ë°°ì› ë‹¤", "ì„±ì¥í–ˆë‹¤", "ë‹¤ìŒì—ëŠ” ë”" â†’ reflection = true

        ---

        ## ê¸°ì—¬ë„ ë¶„ì„:
        - "clear": "ì €ëŠ”", "ì œê°€", "I", "ë‚´ê°€", "ì§ì ‘", "í˜¼ì" ì‚¬ìš©
        - "mixed": "ì €í¬", "ìš°ë¦¬"ì™€ "ì €" í˜¼ìš©
        - "unclear": ì£¼ë¡œ "ìš°ë¦¬", "íŒ€" ìœ„ì£¼ (ê°œì¸ ê¸°ì—¬ ë¶ˆëª…í™•)

        ## êµ¬ì²´ì  ì¦ê±° ì¶”ì¶œ:
        ê¸°ìˆ ëª…, ìˆ˜ì¹˜, ê¸°ê°„, ì„±ê³¼ ì§€í‘œ ë“± êµ¬ì²´ì ì¸ ì •ë³´ ì¶”ì¶œ

        ## Output JSON (ë°˜ë“œì‹œ ì´ í˜•ì‹ìœ¼ë¡œ):
        {{
            "starr": {{
                "situation": true ë˜ëŠ” false,
                "task": true ë˜ëŠ” false,
                "action": true ë˜ëŠ” false,
                "result": true ë˜ëŠ” false,
                "reflection": true ë˜ëŠ” false
            }},
            "contribution": "clear" ë˜ëŠ” "mixed" ë˜ëŠ” "unclear",
            "evidence_clips": ["ì¦ê±°1", "ì¦ê±°2"],
            "answer_quality": "excellent" ë˜ëŠ” "good" ë˜ëŠ” "fair" ë˜ëŠ” "poor"
        }}
        """

        response_text = self._call_llm(prompt)
        result = self._parse_json_response(response_text)

        # LLM ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ fallback ë¶„ì„
        if not result or "starr" not in result:
            print("âš ï¸ LLM STARR analysis failed. Using rule-based fallback.")
            result = self._analyze_answer_fallback(answer)

        # ë‹µë³€ í’ˆì§ˆ ê¸°ë³¸ê°’ ì„¤ì •
        if "answer_quality" not in result:
            starr = result.get("starr", {})
            filled = sum(1 for v in starr.values() if v)
            if filled >= 4:
                result["answer_quality"] = "excellent"
            elif filled >= 3:
                result["answer_quality"] = "good"
            elif filled >= 2:
                result["answer_quality"] = "fair"
            else:
                result["answer_quality"] = "poor"

        return result

    def _analyze_answer_fallback(self, answer: str) -> Dict[str, Any]:
        """
        LLM ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ STARR ë¶„ì„ (ë¬¸ë§¥ ì¸ì‹ - í™•ì¥íŒ)

        ì§ì ‘ í‚¤ì›Œë“œê°€ ì•„ë‹Œ ì˜ë¯¸ì  íŒ¨í„´ìœ¼ë¡œ ì¸ì‹:
        - Situation: ë°°ê²½/ìƒí™©/ì‹œê¸°/ì¥ì†Œ ì–¸ê¸‰
        - Task: ëª©í‘œ/ì—­í• /í•´ì•¼ í•  ì¼ ì–¸ê¸‰
        - Action: í–‰ë™/êµ¬í˜„/ì‚¬ìš© ì–¸ê¸‰
        - Result: ê²°ê³¼/ì„±ê³¼/ì™„ë£Œ ì–¸ê¸‰
        - Reflection: ë°°ì›€/ëŠë‚Œ/ë‹¤ì§ ì–¸ê¸‰
        """
        if not answer:
            return {
                "starr": {"situation": False, "task": False, "action": False, "result": False, "reflection": False},
                "contribution": "unclear",
                "evidence_clips": [],
                "answer_quality": "poor"
            }

        # ========================================
        # Situation íŒ¨í„´ (ë°°ê²½/ìƒí™©/ë§¥ë½/ì‹œê¸°/ì¥ì†Œ)
        # ========================================
        situation_patterns = [
            # ì‹œê°„/ì‹œê¸° í‘œí˜„
            r"ë‹¹ì‹œ", r"ê·¸ë•Œ", r"ë•Œ", r"ê¸°ê°„", r"ë™ì•ˆ", r"ì¤‘ì—", r"ë¬´ë µ", r"ì‹œì ˆ",
            r"ë…„ë„", r"í•™ê¸°", r"í•™ë…„", r"ê°œì›”", r"ì£¼ê°„", r"ì¼ê°„",
            r"ì‘ë…„", r"ì˜¬í•´", r"ì§€ë‚œ", r"ìµœê·¼", r"ì˜ˆì „", r"ê³¼ê±°",
            r"ì´ˆê¸°", r"ì¤‘ë°˜", r"í›„ë°˜", r"ë§", r"ì´ˆ",

            # ì¥ì†Œ/ì¡°ì§ í‘œí˜„
            r"í”„ë¡œì íŠ¸", r"íšŒì‚¬", r"íŒ€", r"ë¶€ì„œ", r"ì¡°ì§", r"ê·¸ë£¹",
            r"í•™êµ", r"ëŒ€í•™", r"ëŒ€í•™êµ", r"í•™ì›", r"êµìœ¡", r"ìº í”„", r"ë¶€íŠ¸ìº í”„",
            r"ì¸í„´", r"ì¸í„´ì‹­", r"í˜„ì¥ì‹¤ìŠµ", r"ì‹¤ìŠµ",
            r"ìŠ¤íƒ€íŠ¸ì—…", r"ê¸°ì—…", r"ë²•ì¸", r"ì„¼í„°", r"ì—°êµ¬ì†Œ", r"ì—°êµ¬ì‹¤",
            r"ë™ì•„ë¦¬", r"í•™íšŒ", r"ëª¨ì„", r"ì»¤ë®¤ë‹ˆí‹°",

            # ìƒí™©/ë§¥ë½ í‘œí˜„
            r"ìƒí™©", r"ë°°ê²½", r"ê³„ê¸°", r"ë§¥ë½", r"í™˜ê²½", r"ì—¬ê±´", r"ì¡°ê±´",
            r"ì—ì„œ", r"ì—ì„ ", r"ì—ì„œëŠ”", r"ì—ì„œì˜",
            r"ì²˜ìŒ", r"ì‹œì‘", r"ì´ˆë°˜", r"ì…ì‚¬", r"ì…í•™", r"í•©ë¥˜",
            r"ë‹¹ë©´", r"ì§ë©´", r"ë§ˆì£¼", r"ê²ª", r"ë°œìƒ", r"ìƒê¸°",

            # í”„ë¡œì íŠ¸/ì—…ë¬´ ìœ í˜•
            r"ì¡¸ì—…", r"ìº¡ìŠ¤í†¤", r"í† ì´", r"ì‚¬ì´ë“œ", r"ê°œì¸", r"íŒ€",
            r"ì™¸ì£¼", r"ìš©ì—­", r"SI", r"SM", r"ìš´ì˜", r"ìœ ì§€ë³´ìˆ˜",
            r"ì‹ ê·œ", r"ë¦¬ë‰´ì–¼", r"ë§ˆì´ê·¸ë ˆì´ì…˜", r"ê³ ë„í™”",
        ]
        has_situation = any(re.search(p, answer) for p in situation_patterns)

        # ========================================
        # Task íŒ¨í„´ (ê³¼ì œ/ì—­í• /ëª©í‘œ/ì±…ì„)
        # ========================================
        task_patterns = [
            # ì—­í• /ë‹´ë‹¹ í‘œí˜„
            r"ë‹´ë‹¹", r"ë§¡", r"ì—­í• ", r"ì±…ì„", r"ì„ë¬´", r"ì—…ë¬´", r"ì¼",
            r"ë¦¬ë“œ", r"ë¦¬ë”", r"ë§¤ë‹ˆì €", r"PM", r"PL", r"TL",
            r"ë©”ì¸", r"ì„œë¸Œ", r"ë³´ì¡°", r"ì§€ì›",
            r"í”„ë¡ íŠ¸", r"ë°±ì—”ë“œ", r"í’€ìŠ¤íƒ", r"ë°ë¸Œì˜µìŠ¤", r"QA", r"ê¸°íš",

            # ëª©í‘œ/ê³¼ì œ í‘œí˜„
            r"ëª©í‘œ", r"ê³¼ì œ", r"ë¯¸ì…˜", r"íƒœìŠ¤í¬", r"task", r"ëª©ì ",
            r"ìš”êµ¬ì‚¬í•­", r"ìš”ì²­", r"ë‹ˆì¦ˆ", r"needs",
            r"í•´ì•¼", r"í•„ìš”", r"ìš”êµ¬", r"ì›í•˜", r"ë°”ë¼",
            r"í•˜ê²Œ ë˜", r"ì„ í•˜ê²Œ", r"ë¥¼ í•˜ê²Œ", r"ë§¡ê²Œ",

            # ë¬¸ì œ/ì´ìŠˆ í‘œí˜„
            r"ë¬¸ì œ", r"ì´ìŠˆ", r"ë²„ê·¸", r"ì˜¤ë¥˜", r"ì—ëŸ¬", r"ì¥ì• ",
            r"ê°œì„ ", r"ë³´ì™„", r"ìˆ˜ì •", r"ë³€ê²½", r"ë¦¬íŒ©í† ë§", r"ë¦¬íŒ©í„°ë§",
            r"ìµœì í™”", r"íŠœë‹", r"ì„±ëŠ¥", r"ì†ë„",

            # ê¸°ëŠ¥/ê°œë°œ ëŒ€ìƒ
            r"ê¸°ëŠ¥", r"ëª¨ë“ˆ", r"ì»´í¬ë„ŒíŠ¸", r"í˜ì´ì§€", r"í™”ë©´", r"ë·°",
            r"API", r"ì—”ë“œí¬ì¸íŠ¸", r"ì„œë¹„ìŠ¤", r"ì‹œìŠ¤í…œ", r"í”Œë«í¼",
            r"ì•±", r"ì–´í”Œ", r"ì›¹", r"ì‚¬ì´íŠ¸", r"í¬í„¸",
            r"ë§Œë“¤", r"êµ¬í˜„í•´ì•¼", r"ê°œë°œí•´ì•¼", r"ì‘ì„±í•´ì•¼",
        ]
        has_task = any(re.search(p, answer) for p in task_patterns)

        # ========================================
        # Action íŒ¨í„´ (í–‰ë™/ì‹¤í–‰/êµ¬í˜„/ê¸°ìˆ  ì‚¬ìš©)
        # ========================================
        action_patterns = [
            # í–‰ë™ ë™ì‚¬ (ê³¼ê±°í˜• í¬í•¨)
            r"í–ˆìŠµë‹ˆë‹¤", r"í–ˆì–´ìš”", r"í–ˆê³ ", r"í–ˆëŠ”ë°", r"í–ˆìœ¼ë©°",
            r"í•˜ì—¬", r"í•´ì„œ", r"í•˜ê³ ", r"í•¨ìœ¼ë¡œì¨",
            r"í–ˆë˜", r"í•œ ì ", r"í•´ë³¸", r"í•´ë´¤",
            r"ì§„í–‰", r"ìˆ˜í–‰", r"ì‹¤í–‰", r"ì‹¤ì‹œ", r"ì‹œí–‰",

            # ê°œë°œ/êµ¬í˜„ ë™ì‚¬
            r"êµ¬í˜„", r"ê°œë°œ", r"ì‘ì„±", r"ì½”ë”©", r"í”„ë¡œê·¸ë˜ë°",
            r"ì„¤ê³„", r"ë””ìì¸", r"ì•„í‚¤í…ì²˜", r"êµ¬ì¡°í™”",
            r"ì ìš©", r"ë„ì…", r"ì±„íƒ", r"ì‚¬ìš©", r"í™œìš©", r"ì´ìš©",
            r"ì—°ë™", r"í†µí•©", r"ì¸í„°í˜ì´ìŠ¤", r"ì—°ê²°",
            r"ë°°í¬", r"ë¦´ë¦¬ì¦ˆ", r"ëŸ°ì¹­", r"ì˜¤í”ˆ",

            # ìˆ˜ì •/ê°œì„  ë™ì‚¬
            r"ìˆ˜ì •", r"ë³€ê²½", r"ì—…ë°ì´íŠ¸", r"íŒ¨ì¹˜",
            r"ì¶”ê°€", r"ì‚­ì œ", r"ì œê±°", r"ì‚½ì…",
            r"ë¦¬íŒ©í† ë§", r"ë¦¬íŒ©í„°ë§", r"ì •ë¦¬", r"í´ë¦°ì—…",
            r"ìµœì í™”", r"íŠœë‹", r"ê°œì„ ",

            # í…ŒìŠ¤íŠ¸/ê²€ì¦ ë™ì‚¬
            r"í…ŒìŠ¤íŠ¸", r"ê²€ì¦", r"í™•ì¸", r"ê²€í† ", r"ë¦¬ë·°",
            r"QA", r"ë””ë²„ê¹…", r"ë””ë²„ê·¸", r"íŠ¸ëŸ¬ë¸”ìŠˆíŒ…",

            # í˜‘ì—…/ì†Œí†µ ë™ì‚¬
            r"í˜‘ì—…", r"í˜‘ë ¥", r"ì†Œí†µ", r"ì»¤ë®¤ë‹ˆì¼€ì´ì…˜",
            r"íšŒì˜", r"ë¯¸íŒ…", r"ë…¼ì˜", r"í† ë¡ ", r"ì œì•ˆ",
            r"ê³µìœ ", r"ì „ë‹¬", r"ë³´ê³ ", r"ë°œí‘œ",

            # ê¸°ìˆ  ìŠ¤íƒ (í”„ë ˆì„ì›Œí¬/ì–¸ì–´/ë„êµ¬)
            # Frontend
            r"React", r"Vue", r"Angular", r"Svelte", r"Next", r"Nuxt",
            r"JavaScript", r"TypeScript", r"jQuery", r"HTML", r"CSS", r"SCSS", r"Sass",
            r"Redux", r"Recoil", r"Zustand", r"MobX", r"Vuex", r"Pinia",
            r"Webpack", r"Vite", r"Babel", r"ESLint", r"Prettier",

            # Backend
            r"Spring", r"SpringBoot", r"Spring Boot", r"JPA", r"Hibernate",
            r"Django", r"Flask", r"FastAPI", r"Express", r"NestJS", r"Koa",
            r"Node", r"NodeJS", r"Node\.js", r"Deno",
            r"Ruby", r"Rails", r"Laravel", r"PHP", r"ASP\.NET", r"\.NET",

            # Languages
            r"Java", r"Python", r"Kotlin", r"Swift", r"Go", r"Golang", r"Rust",
            r"C\+\+", r"C#", r"Scala", r"Clojure",

            # Database
            r"MySQL", r"PostgreSQL", r"Postgres", r"MariaDB", r"Oracle", r"MSSQL",
            r"MongoDB", r"Redis", r"Elasticsearch", r"DynamoDB", r"Cassandra",
            r"SQL", r"NoSQL", r"ì¿¼ë¦¬", r"ì¸ë±ìŠ¤", r"ì •ê·œí™”",

            # DevOps/Infra
            r"AWS", r"Azure", r"GCP", r"í´ë¼ìš°ë“œ", r"Cloud",
            r"Docker", r"Kubernetes", r"K8s", r"ì»¨í…Œì´ë„ˆ",
            r"Jenkins", r"GitLab", r"GitHub Actions", r"CI/CD", r"CI", r"CD",
            r"Terraform", r"Ansible", r"Nginx", r"Apache",

            # Mobile
            r"Android", r"iOS", r"Flutter", r"React Native", r"Xamarin",

            # etc
            r"Git", r"SVN", r"Jira", r"Confluence", r"Slack", r"Notion",
            r"Figma", r"Zeplin", r"Swagger", r"Postman",
            r"REST", r"GraphQL", r"gRPC", r"WebSocket", r"ì†Œì¼“",
            r"JWT", r"OAuth", r"ì¸ì¦", r"ë³´ì•ˆ",
            r"ë¡œê·¸", r"ëª¨ë‹ˆí„°ë§", r"ì•Œë¦¼", r"ëŒ€ì‹œë³´ë“œ",
        ]
        has_action = any(re.search(p, answer, re.IGNORECASE) for p in action_patterns)

        # ========================================
        # Result íŒ¨í„´ (ê²°ê³¼/ì„±ê³¼/ì™„ë£Œ/íš¨ê³¼)
        # ========================================
        result_patterns = [
            # ì™„ë£Œ/ì„±ê³µ í‘œí˜„
            r"ì™„ë£Œ", r"ì™„ì„±", r"ë§ˆë¬´ë¦¬", r"ì¢…ë£Œ", r"ë",
            r"ì„±ê³µ", r"ì„±ê³µì ", r"ë¬´ì‚¬íˆ", r"ì •ìƒì ",
            r"ì¶œì‹œ", r"ì˜¤í”ˆ", r"ëŸ°ì¹­", r"ë°°í¬", r"ë¦´ë¦¬ì¦ˆ",
            r"ë‚©í’ˆ", r"ì¸ìˆ˜ì¸ê³„", r"ì´ê´€",

            # ì„±ê³¼/ê²°ê³¼ í‘œí˜„
            r"ê²°ê³¼", r"ì„±ê³¼", r"íš¨ê³¼", r"ì˜í–¥", r"ì„íŒ©íŠ¸",
            r"ë‹¬ì„±", r"ë„ë‹¬", r"ì¶©ì¡±", r"ë§Œì¡±",
            r"í•´ê²°", r"ì²˜ë¦¬", r"ê·¹ë³µ", r"ëŒíŒŒ",

            # ê°œì„ /í–¥ìƒ í‘œí˜„
            r"ê°œì„ ", r"í–¥ìƒ", r"ì¦ê°€", r"ìƒìŠ¹", r"ì˜¬ë¼",
            r"ê°ì†Œ", r"ì¤„", r"ë‚®ì¶”", r"ì ˆê°", r"ë‹¨ì¶•",
            r"ë¹¨ë¼", r"ëŠë ¤", r"ì¢‹ì•„", r"ë‚˜ì•„",

            # ìˆ˜ì¹˜/ì§€í‘œ í‘œí˜„
            r"\d+%", r"\d+í¼ì„¼íŠ¸", r"\d+ë°°", r"\d+å€",
            r"\d+ëª…", r"\d+ê±´", r"\d+ê°œ", r"\d+íšŒ",
            r"\d+ì´ˆ", r"\d+ms", r"\d+ë¶„", r"\d+ì‹œê°„",
            r"TPS", r"RPS", r"QPS", r"DAU", r"MAU", r"PV", r"UV",
            r"íŠ¸ë˜í”½", r"ì²˜ë¦¬ëŸ‰", r"ì‘ë‹µì‹œê°„", r"ë ˆì´í„´ì‹œ", r"latency",

            # ìƒíƒœ ë³€í™” í‘œí˜„
            r"ë˜ì—ˆ", r"ë", r"ë¨", r"ë˜ì–´", r"ë¼",
            r"ë°›ì•˜", r"ì–»ì—ˆ", r"ê°€ì ¸", r"í™•ë³´",
            r"ì¸ì •", r"í‰ê°€", r"í˜¸í‰", r"ì¢‹ì€ ë°˜ì‘",

            # ìˆ˜ìƒ/ì¸ì •
            r"ìˆ˜ìƒ", r"ìƒ", r"1ë“±", r"1ìœ„", r"ìš°ìˆ˜", r"ìµœìš°ìˆ˜",
            r"ì„ ì •", r"ì±„íƒ", r"í•©ê²©", r"í†µê³¼",
        ]
        has_result = any(re.search(p, answer) for p in result_patterns)

        # ========================================
        # Reflection íŒ¨í„´ (ì„±ì°°/ë°°ì›€/ëŠë‚Œ/ë‹¤ì§)
        # ========================================
        reflection_patterns = [
            # ë°°ì›€/ê¹¨ë‹¬ìŒ í‘œí˜„
            r"ë°°ì› ", r"ë°°ìš´", r"ë°°ìš°ê²Œ", r"ìŠµë“", r"ìµíˆ",
            r"ëŠê¼ˆ", r"ëŠë‚€", r"ëŠë¼ê²Œ", r"ì²´ê°",
            r"ê¹¨ë‹¬", r"ì•Œê²Œ", r"ì´í•´í•˜ê²Œ", r"íŒŒì•…í•˜ê²Œ",
            r"ê²½í—˜", r"ì²´í—˜", r"ê²ªìœ¼ë©´ì„œ", r"í†µí•´",

            # ì„±ì¥/ë°œì „ í‘œí˜„
            r"ì„±ì¥", r"ë°œì „", r"í–¥ìƒ", r"ëŠ˜", r"ë‚˜ì•„",
            r"ì—­ëŸ‰", r"ëŠ¥ë ¥", r"ìŠ¤í‚¬", r"skill",
            r"ìì‹ ê°", r"í™•ì‹ ", r"ë¯¿ìŒ",

            # ë°˜ì„±/ë¶€ì¡± í‘œí˜„
            r"ë¶€ì¡±", r"ë¯¸í¡", r"ì•„ì‰¬", r"í›„íšŒ", r"ë°˜ì„±",
            r"ëª°ë", r"ëª¨ë¥´", r"ì‹¤ìˆ˜", r"ì˜ëª»",
            r"ì–´ë ¤", r"í˜ë“¤", r"ì–´ë µ",
            r"í•œê³„", r"ì œí•œ", r"ë¬¸ì œì ",

            # ë‹¤ì§/ê³„íš í‘œí˜„
            r"ë‹¤ìŒì—ëŠ”", r"ì•ìœ¼ë¡œ", r"í–¥í›„", r"ì´í›„",
            r"ë…¸ë ¥", r"ì‹œë„", r"ë„ì „", r"ê³„íš",
            r"ëª©í‘œ", r"ë°©í–¥", r"ë¹„ì „",
            r"ë” ì˜", r"ë” ë‚˜ì€", r"ê°œì„ í•˜ê³ ",

            # êµí›ˆ/ì¸ì‚¬ì´íŠ¸ í‘œí˜„
            r"êµí›ˆ", r"ì¸ì‚¬ì´íŠ¸", r"insight", r"ì‹œì‚¬ì ",
            r"ê¹¨ë‹«", r"í„°ë“", r"ì²´ë“",
            r"ì¤‘ìš”ì„±", r"í•„ìš”ì„±", r"ê°€ì¹˜",
            r"ë•ë¶„ì—", r"ê³„ê¸°ë¡œ", r"ê¸°íšŒ",

            # ìƒê°/ì˜ê²¬ í‘œí˜„
            r"ìƒê°", r"íŒë‹¨", r"ì˜ê²¬", r"ê²¬í•´",
            r"ê°™ìŠµë‹ˆë‹¤", r"ë´…ë‹ˆë‹¤", r"ëŠë‚ë‹ˆë‹¤",
            r"ê²ƒ ê°™", r"ë“¯ í•©ë‹ˆë‹¤", r"ë“¯í•©ë‹ˆë‹¤",
        ]
        has_reflection = any(re.search(p, answer) for p in reflection_patterns)

        # ========================================
        # ê¸°ì—¬ë„ ë¶„ì„ (ê°œì¸ vs íŒ€)
        # ========================================
        contribution = "unclear"

        # ëª…í™•í•œ ê°œì¸ ê¸°ì—¬ í‘œí˜„
        clear_individual = [
            r"ì €ëŠ”", r"ì œê°€", r"ì €ì˜", r"ì œ", r"ë³¸ì¸",
            r"I ", r"I'm", r"my ", r"mine",
            r"ë‚´ê°€", r"ë‚˜ëŠ”", r"ë‚˜ì˜",
            r"ì§ì ‘", r"ìŠ¤ìŠ¤ë¡œ", r"í˜¼ì",
            r"ì£¼ë„", r"ë¦¬ë“œ", r"ì´ëŒ",
        ]

        # íŒ€/ê·¸ë£¹ í‘œí˜„
        team_expressions = [
            r"ì €í¬", r"ìš°ë¦¬", r"íŒ€", r"ê·¸ë£¹", r"ì¡°",
            r"we ", r"our ", r"us ",
            r"ê°™ì´", r"í•¨ê»˜", r"í˜‘ë ¥", r"í˜‘ì—…",
            r"ë™ë£Œ", r"íŒ€ì›", r"ë©¤ë²„",
        ]

        has_individual = any(re.search(p, answer, re.IGNORECASE) for p in clear_individual)
        has_team = any(re.search(p, answer, re.IGNORECASE) for p in team_expressions)

        if has_individual and not has_team:
            contribution = "clear"
        elif has_individual and has_team:
            contribution = "mixed"
        elif has_team and not has_individual:
            contribution = "unclear"
        else:
            # ë‘˜ ë‹¤ ì—†ìœ¼ë©´ ë™ì‚¬ í˜•íƒœë¡œ ì¶”ì •
            if re.search(r"í–ˆìŠµë‹ˆë‹¤|í–ˆì–´ìš”|í–ˆê³ |ë§Œë“¤ì—ˆ|ê°œë°œí–ˆ|êµ¬í˜„í–ˆ", answer):
                contribution = "mixed"  # ì£¼ì–´ ìƒëµëœ ê²½ìš°

        # ========================================
        # êµ¬ì²´ì  ì¦ê±° ì¶”ì¶œ (ê¸°ìˆ ëª…, ìˆ˜ì¹˜, ì„±ê³¼)
        # ========================================
        evidence_clips = []

        # ê¸°ìˆ  ìŠ¤íƒ ì¶”ì¶œ
        tech_patterns = r"(React|Vue|Angular|Svelte|Next\.?js|Nuxt|Spring|SpringBoot|Django|Flask|FastAPI|Express|NestJS|Node\.?js|Java|Python|Kotlin|Swift|Go|Rust|TypeScript|JavaScript|MySQL|PostgreSQL|MongoDB|Redis|AWS|Azure|GCP|Docker|Kubernetes|Git|Jenkins|GraphQL|REST)"
        tech_matches = re.findall(tech_patterns, answer, re.IGNORECASE)
        evidence_clips.extend(list(set(tech_matches)))

        # ìˆ˜ì¹˜/ì§€í‘œ ì¶”ì¶œ
        num_patterns = [
            r"(\d+(?:\.\d+)?%)",  # í¼ì„¼íŠ¸
            r"(\d+(?:\.\d+)?ë°°)",  # ë°°ìˆ˜
            r"(\d+(?:,\d+)?ëª…)",  # ì¸ì›
            r"(\d+(?:,\d+)?ê±´)",  # ê±´ìˆ˜
            r"(\d+(?:,\d+)?ê°œ)",  # ê°œìˆ˜
            r"(\d+(?:\.\d+)?ì´ˆ)",  # ì‹œê°„(ì´ˆ)
            r"(\d+(?:\.\d+)?ms)",  # ë°€ë¦¬ì´ˆ
            r"(\d+(?:,\d+)?ì›)",  # ê¸ˆì•¡
            r"(\d+(?:\.\d+)?GB)",  # ìš©ëŸ‰
            r"(\d+(?:\.\d+)?MB)",  # ìš©ëŸ‰
        ]
        for pattern in num_patterns:
            matches = re.findall(pattern, answer)
            evidence_clips.extend(matches)

        return {
            "starr": {
                "situation": has_situation,
                "task": has_task,
                "action": has_action,
                "result": has_result,
                "reflection": has_reflection
            },
            "contribution": contribution,
            "evidence_clips": evidence_clips[:10]  # ìµœëŒ€ 10ê°œ
        }

    def _determine_reaction_type(self, analysis: Dict[str, Any]) -> str:
        """
        ë‹µë³€ í’ˆì§ˆì— ë”°ë¥¸ ë©´ì ‘ê´€ ë°˜ì‘ íƒ€ì… ê²°ì •
        - satisfied: STARR 4ê°œ ì´ìƒ, ê¸°ì—¬ë„ ëª…í™•
        - impressed: STARR 5ê°œ ì™„ë²½, ì¦ê±° í’ë¶€
        - good: STARR 3ê°œ, ê´œì°®ì€ ìˆ˜ì¤€
        - neutral: STARR 2ê°œ
        - concerned: STARR 1ê°œ ì´í•˜, ê¸°ì—¬ë„ ë¶ˆëª…í™•
        - unsatisfied: ê±°ì˜ ë‚´ìš© ì—†ìŒ
        """
        starr = analysis.get("starr", {})
        filled = sum(1 for v in starr.values() if v)
        contribution = analysis.get("contribution", "unclear")
        evidence = analysis.get("evidence_clips", [])
        quality = analysis.get("answer_quality", "fair")

        # í’ˆì§ˆ ê¸°ë°˜ íŒë‹¨
        if quality == "excellent" or (filled >= 4 and contribution == "clear" and len(evidence) >= 2):
            return "impressed"
        elif quality == "good" or (filled >= 3 and contribution in ["clear", "mixed"]):
            return "satisfied"
        elif filled >= 2:
            return "good" if contribution != "unclear" else "neutral"
        elif filled == 1:
            return "concerned"
        else:
            return "unsatisfied"

    def build_probe(self, analysis: Dict[str, Any], role: str, last_question: str, last_answer: str, difficulty: str = "JUNIOR") -> Dict[str, Any]:
        starr = analysis.get("starr", {})

        # Determine strategy based on missing components
        if not starr.get("action"):
            strategy = "clarify_action"
        elif not starr.get("result"):
            strategy = "clarify_result"
        elif analysis.get("contribution") == "unclear":
            strategy = "clarify_contribution"
        elif not starr.get("reflection"):
            strategy = "reflect"
        else:
            strategy = "paraphrase_and_deepen"

        # ë‹µë³€ í’ˆì§ˆì— ë”°ë¥¸ ë°˜ì‘ íƒ€ì… ê²°ì •
        reaction_type = self._determine_reaction_type(analysis)

        difficulty_instruction = ""
        if difficulty == "SENIOR":
            difficulty_instruction = "Challenge the candidate on their decisions. Ask 'Why did you choose X over Y?' or about trade-offs."
        else:
            difficulty_instruction = "Encourage them to explain their thought process clearly."

        prompt = f"""
        You are a technical interviewer for a {role} position.
        Difficulty: {difficulty}. {difficulty_instruction}

        Context:
        - Previous Question: "{last_question}"
        - Candidate Answer: "{last_answer}"
        - Analysis Status: {json.dumps(analysis, ensure_ascii=False)}
        - Chosen Strategy: {strategy}

        Task:
        Generate a follow-up question in Korean. The question only, no reaction text needed.
        **CRITICAL: NEVER use the words "STAR", "STARR", "ìŠ¤íƒ€", or "STAR ë°©ì‹" in your question.**

        Output JSON:
        {{
            "next_question": "The follow-up question in Korean",
            "probe_goal": "Goal of this follow-up",
            "requested_evidence": ["list of items to verify"]
        }}
        """

        response_text = self._call_llm(prompt)
        data = self._parse_json_response(response_text)

        # Fallback if LLM fails
        if not data:
            return {
                "next_question": "êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ê¸°ìˆ ì  ì–´ë ¤ì›€ì´ ìˆì—ˆëŠ”ì§€ ì„¤ëª…í•´ ì£¼ì„¸ìš”.",
                "reaction": {"type": reaction_type, "text": ""},
                "probe_goal": "ìƒì„¸ ë‚´ìš© í™•ì¸",
                "requested_evidence": []
            }

        return {
            "next_question": data.get("next_question"),
            "reaction": {"type": reaction_type, "text": ""},  # text ë¹„ì›€ - í”„ë¡ íŠ¸ì—ì„œ íƒ€ì…ë§Œ ì‚¬ìš©
            "probe_goal": data.get("probe_goal"),
            "requested_evidence": data.get("requested_evidence", [])
        }

    def _score_from_starr(self, starr: Dict[str, bool]) -> float:
        score = 3.0
        if starr.get("situation"): score += 0.3
        if starr.get("task"): score += 0.3
        if starr.get("action"): score += 0.5
        if starr.get("result"): score += 0.5
        if starr.get("reflection"): score += 0.4
        return min(5.0, score)

    def build_report(self, role: str, analysis: Dict[str, Any]) -> Dict[str, Any]:
        # Using LLM to generate qualitative feedback is better, but keeping it simple for now to align with existing frontend expectations
        # that roughly map to the previous structure.
        
        starr = analysis.get("starr", {})
        score = self._score_from_starr(starr)
        
        prompt = f"""
        Generate a brief feedback comment (Korean) for a candidate based on this analysis:
        Analysis: {json.dumps(analysis, ensure_ascii=False)}
        Score: {score}/5.0
        
        Output JSON:
        {{
            "feedback_level": "High" | "Mid" | "Low",
            "feedback_comment": "One sentence summary"
        }}
        """
        response = self._parse_json_response(self._call_llm(prompt))
        
        return {
            "role": role,
            "competency_scores": { "general": score }, # Simplified for now
            "starr_coverage": starr,
            "individual_contribution": analysis.get("contribution"),
            "strengths": analysis.get("evidence_clips", []), # Using evidence as strengths for now
            "gaps": [k for k, v in starr.items() if not v],
            "feedback_level": response.get("feedback_level", "Mid"),
            "feedback_comment": response.get("feedback_comment", "ì „ë°˜ì ìœ¼ë¡œ ê´œì°®ìœ¼ë‚˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì´ ë” í•„ìš”í•©ë‹ˆë‹¤."),
            "evidence_clips": analysis.get("evidence_clips", [])
        }

    def generate_response(self, resume_input: Optional[Dict[str, Any]], portfolio: Optional[Dict[str, Any]], last_answer: Optional[str], portfolio_files: Optional[List[str]] = None, total_turns: int = 5, chat_history: Optional[List[Dict[str, Any]]] = None, difficulty: str = "JUNIOR") -> Dict[str, Any]:
        
        # [NEW] Stateless Support: Hydrate State from History
        if chat_history is not None:
            # Deep copy to avoid reference issues
            self.chat_history = [item.copy() for item in chat_history]
            print(f"ğŸ”„ Hydrated chat_history from request: {len(self.chat_history)} items")
            
            # [FIX] Defensive Parsing for System messages
            for item in self.chat_history:
                if item.get("role") == "system" and isinstance(item.get("content"), str):
                    try:
                        item["content"] = json.loads(item["content"])
                    except:
                        pass

            # Re-determine Phase based on history
            assistant_questions = [h for h in self.chat_history if h.get("role") == "assistant" and h.get("type") == "question"]
            question_count = len(assistant_questions)
            
            if question_count == 0:
                self.current_phase = "INTRO"
            elif question_count >= total_turns - 1:
                self.current_phase = "CLOSING"
            else:
                self.current_phase = "MAIN"
                
            # [FIX] Restore current_topic_probe_count
            # Scan backwards from the end. Count questions until we hit a "seed" question or start of MAIN.
            # Assuming metadata contains "reaction" type. "transition" or "acknowledge" usually reset the count.
            # If metadata is missing, assumes 0.
            probe_c = 0
            if self.current_phase == "MAIN":
                for item in reversed(self.chat_history):
                    if item.get("role") == "assistant" and item.get("type") == "question":
                        meta = item.get("metadata", {})
                        if isinstance(meta, str): # Handle string metadata if any
                             try: meta = json.loads(meta)
                             except: meta = {}
                        
                        reaction_type = meta.get("reaction", {}).get("type", "")
                        if reaction_type in ["transition", "acknowledge", "welcome"]:
                            # This was a seed question, so we stop counting here (this is count 0 base)
                            break
                        else:
                            # It was a probe/clarify/reflect
                            probe_c += 1
            self.current_topic_probe_count = probe_c
                
            print(f"ğŸ”„ State Restored: Phase={self.current_phase}, Question Count={question_count}, Probe Count={self.current_topic_probe_count}")
        
        # 1. Start Interview (INTRO Phase)
        if not self.chat_history:
            self.context["resume"] = resume_input or {}
            self.context["portfolio"] = portfolio or {}
            self.context["total_turns"] = total_turns # âœ… ì „ì²´ íšŸìˆ˜ ì €ì¥
            self.context["difficulty"] = difficulty # âœ… ë‚œì´ë„ ì €ì¥
            
            # --- Portfolio File Parsing ---
            portfolio_text = ""
            files_to_parse = portfolio_files or []
            if not files_to_parse and portfolio and "files" in portfolio:
                files_to_parse = portfolio["files"]
            
            if files_to_parse:
                print(f"ğŸ“‚ Processing {len(files_to_parse)} portfolio files...")
                for file_path in files_to_parse:
                    parsed = FileParser.parse_file(file_path)
                    portfolio_text += f"\n--- File: {os.path.basename(file_path)} ---\n{parsed}\n"
            
            if portfolio_text:
                self.context["portfolio_parsed_text"] = portfolio_text
                print(f"âœ… Portfolio Parsed Length: {len(portfolio_text)} chars")
            
            target_role = (
                self.context["resume"].get("classification", {}).get("predicted_role")
                or self.context["resume"].get("target_role")
            )
            self.context["role"] = self.normalize_role(target_role)
            
            # [NEW] Start with INTRO question (ìê¸°ì†Œê°œ)
            self.current_phase = "INTRO"
            question, probe_goal, requested_evidence = self.build_intro_question(
                self.context["role"], 
                self.context["resume"].get("resume_content"),
                difficulty
            )
            print(f"ğŸ¬ [Phase: INTRO] Starting interview with introduction question ({difficulty})")
            
            response_data = {
                "next_question": question,
                "reaction": {
                    "type": "welcome",
                    "text": ""  # í…ìŠ¤íŠ¸ ë¹„ì›€ - í”„ë¡ íŠ¸ì—ì„œ íƒ€ì…ë§Œ ì‚¬ìš©
                },
                "probe_goal": probe_goal,
                "requested_evidence": requested_evidence,
                "report": None,
                "phase": self.current_phase
            }
            
            self.chat_history.append({
                "role": "assistant",
                "type": "question",
                "content": question,
                "metadata": response_data,
                "phase": self.current_phase
            })
            
            return response_data

        # 2. Continue Interview
        
        # [FIX] Prevent Duplicate Answer
        # Check if the last item in history is ALREADY the same as last_answer
        is_duplicate = False
        if self.chat_history and last_answer:
            last_item = self.chat_history[-1]
            if last_item.get("role") == "user" and last_item.get("content") == last_answer:
                is_duplicate = True
                print("âš ï¸ [State] last_answer already exists in history. Skipping append.")
        
        if last_answer and not is_duplicate:
            self.chat_history.append({
                "role": "user",
                "content": last_answer
            })


        # Get context of previous question
        last_question_item = next((item for item in reversed(self.chat_history) if item["role"] == "assistant"), None)
        last_question_text = last_question_item["content"] if last_question_item else ""

        # Analyze answer
        analysis = self.analyze_answer(last_answer or "")
        self.chat_history.append({
            "role": "system",
            "type": "analysis",
            "content": analysis
        })
        
        # Calculate question count
        assistant_questions = [h for h in self.chat_history if h.get("role") == "assistant" and h.get("type") == "question"]
        question_count = len(assistant_questions)
        
        # Ensure context is loaded if hydrated
        if "role" not in self.context and resume_input:
             target_role = (
                resume_input.get("classification", {}).get("predicted_role")
                or resume_input.get("target_role")
            )
             self.context["role"] = self.normalize_role(target_role)
             self.context["resume"] = resume_input
             self.context["portfolio"] = portfolio
             self.context["difficulty"] = difficulty


        print(f"ğŸ“Š [Phase: {self.current_phase}] Question #{question_count}, Probe count: {self.current_topic_probe_count}, Difficulty: {difficulty}")

        # [NEW] Phase Transition Logic
        # Case 1: INTRO -> MAIN Transition
        # If we are in INTRO phase limit (question_count=0), current_phase is INTRO.
        # If we hydrated and found 1 question (The intro), current_phase became MAIN.
        # BUT we must treat "Question #1 Answered" as the trigger for the First SEED Question.
        
        if self.current_phase == "INTRO":
             # Legacy path if hydration didn't switch it
             self.current_phase = "MAIN"
             # ...
             
        # [FIX] Implicit Transition check:
        # If we are in MAIN phase, but question_count is exactly 1 (Intro asked),
        # meaning we just finished Intro. We MUST generate the first Seed question.
        # AND we should NOT probe the Intro answer.
        
        is_intro_transition = (question_count == 1)
        
        if self.current_phase == "INTRO" or is_intro_transition:
            # After intro answer, move to MAIN phase
            self.current_phase = "MAIN"
            self.current_topic_probe_count = 0
            print(f"â¡ï¸ Transitioning to MAIN phase (Intro Finished)")
            
            # Generate first project question
            # [FIX] Pass previous questions to prevent repeats
            previous_qs = [item["content"] for item in self.chat_history if item.get("role") == "assistant" and item.get("type") == "question"]
            
            question, probe_goal, requested_evidence = self.build_seed_question(
                self.context["role"], 
                self.context["resume"].get("resume_content"), 
                self.context["portfolio"],
                self.context.get("portfolio_parsed_text"),
                difficulty,
                previous_qs # Pass history
            )
            
            # INTRO ë‹µë³€ì— ëŒ€í•œ ë°˜ì‘ íƒ€ì… ê²°ì •
            intro_reaction_type = self._determine_reaction_type(analysis)

            response_data = {
                "next_question": question,
                "reaction": {"type": intro_reaction_type, "text": ""},  # í…ìŠ¤íŠ¸ ë¹„ì›€
                "probe_goal": probe_goal,
                "requested_evidence": requested_evidence,
                "report": self.build_report(self.context.get("role", "backend"), analysis),
                "phase": self.current_phase,
                "analysis_result": analysis  # ë¶„ì„ ê²°ê³¼ í¬í•¨
            }
            
        elif self.current_phase == "COMPLETED" or question_count >= total_turns - 1:
            self.current_phase = "COMPLETED"
            # Return Goodbye Message
            report = self.build_report(self.context.get("role", "backend"), analysis)
            response_data = {
                "next_question": "ë©´ì ‘ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤. (ì ì‹œ í›„ ê²°ê³¼ í™”ë©´ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤)",
                "reaction": {"type": "complete", "text": ""},  # í…ìŠ¤íŠ¸ ë¹„ì›€
                "probe_goal": "ìµœì¢… ì¢…ë£Œ",
                "requested_evidence": [],
                "report": report,
                "phase": "COMPLETED",
                "analysis_result": analysis
            }
            self.chat_history.append({
                "role": "assistant",
                "type": "question",
                "content": response_data["next_question"],
                "metadata": response_data,
                "phase": "COMPLETED"
            })
            return response_data

        elif self.current_phase == "CLOSING" or question_count == total_turns - 2:
            self.current_phase = "CLOSING"
            # Return Closing Question
            # ì§ì „ ë‹µë³€ì— ëŒ€í•œ ë°˜ì‘ íƒ€ì… ê²°ì •
            closing_reaction_type = self._determine_reaction_type(analysis)

            question, probe_goal, requested_evidence = self.build_closing_question(self.context["role"], difficulty)
            response_data = {
                "next_question": question,
                "reaction": {"type": closing_reaction_type, "text": ""},  # í…ìŠ¤íŠ¸ ë¹„ì›€
                "probe_goal": probe_goal,
                "requested_evidence": requested_evidence,
                "report": self.build_report(self.context.get("role", "backend"), analysis),
                "phase": "CLOSING",
                "analysis_result": analysis
            }
            
        elif self.current_phase == "MAIN":
            # MAIN Phase Logic
            # [NEW] Probe count limiting - prevent infinite follow-ups
            self.current_topic_probe_count += 1
            
            # (Fall through to existing logic below)
                
            starr = analysis.get("starr", {})
            starr_filled = sum(1 for v in starr.values() if v)
            
            # Move to next topic if: probe limit reached OR STARR is sufficiently complete (3+ elements)
            if self.current_topic_probe_count >= self.max_probes_per_topic or starr_filled >= 3:
                print(f"ğŸ”„ Moving to next topic (probes: {self.current_topic_probe_count}, STARR filled: {starr_filled})")
                self.current_topic_probe_count = 0
                
                # Generate new topic question
                # [FIX] Pass previous questions
                previous_qs = [item["content"] for item in self.chat_history if item.get("role") == "assistant" and item.get("type") == "question"]

                question, probe_goal, requested_evidence = self.build_seed_question(
                    self.context["role"], 
                    self.context["resume"].get("resume_content"), 
                    self.context["portfolio"],
                    self.context.get("portfolio_parsed_text"),
                    difficulty,
                    previous_qs # Pass history
                )
                
                # ë‹µë³€ í’ˆì§ˆì— ë”°ë¥¸ ë°˜ì‘ íƒ€ì… ê²°ì •
                transition_reaction_type = self._determine_reaction_type(analysis)

                response_data = {
                    "next_question": question,
                    "reaction": {"type": transition_reaction_type, "text": ""},  # í…ìŠ¤íŠ¸ ë¹„ì›€
                    "probe_goal": probe_goal,
                    "requested_evidence": requested_evidence,
                    "report": self.build_report(self.context.get("role", "backend"), analysis),
                    "phase": self.current_phase,
                    "analysis_result": analysis  # ë¶„ì„ ê²°ê³¼ í¬í•¨
                }
            else:
                # Continue probing same topic
                response_data_dict = self.build_probe(
                    analysis, 
                    self.context.get("role", "backend"), 
                    last_question_text, 
                    last_answer,
                    difficulty
                )
                
                response_data = {
                    "next_question": response_data_dict["next_question"],
                    "reaction": response_data_dict["reaction"],
                    "probe_goal": response_data_dict["probe_goal"],
                    "requested_evidence": response_data_dict["requested_evidence"],
                    "report": self.build_report(self.context.get("role", "backend"), analysis),
                    "phase": self.current_phase,
                    "analysis_result": analysis  # ë¶„ì„ ê²°ê³¼ í¬í•¨
                }

        elif self.current_phase == "CLOSING":
             # Already in closing phase, but backend requested another turn?
             # Just return a polite closing message.
             report = self.build_report(self.context.get("role", "backend"), analysis)
             response_data = {
                "next_question": "ë©´ì ‘ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤.",
                "reaction": {"type": "complete", "text": ""},  # í…ìŠ¤íŠ¸ ë¹„ì›€
                "probe_goal": "ë©´ì ‘ ì¢…ë£Œ",
                "requested_evidence": [],
                "report": report,
                "phase": self.current_phase
            }

        response_data = {
            "next_question": response_data["next_question"],
            "reaction": response_data["reaction"],
            "probe_goal": response_data.get("probe_goal"),
            "requested_evidence": response_data.get("requested_evidence"),
            "report": response_data.get("report"),
            "phase": self.current_phase,
            "analysis_result": analysis # âœ… [NEW] Return raw analysis for stateless storage
        }

        self.chat_history.append({
            "role": "assistant",
            "type": "question",
            "content": response_data["next_question"],
            "metadata": response_data,
            "phase": self.current_phase
        })

        return response_data

    def finalize_interview(self, chat_history: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]:
        # [NEW] Stateless Support
        if chat_history is not None:
             self.chat_history = [item.copy() for item in chat_history]
             print(f"ğŸ”„ Hydrated chat_history for Finalize: {len(self.chat_history)} items")

        print(f"ğŸ [Finalize] chat_history length: {len(self.chat_history)}")
        print(f"ğŸ [Finalize] chat_history roles: {[item.get('role') for item in self.chat_history]}")

        if not self.chat_history:
            print("âŒ [Finalize] No chat history!")
            return {"error": "No interview history found."}

        analyses = []
        for item in self.chat_history:
            if item.get("role") == "system" and item.get("type") == "analysis":
                content = item.get("content")
                if isinstance(content, str):
                    try:
                        content = json.loads(content)
                    except:
                        continue
                if isinstance(content, dict):
                    analyses.append(content)

        print(f"ğŸ [Finalize] Found {len(analyses)} analysis records")

        if not analyses:
            # ë¶„ì„ ë°ì´í„°ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ê°’ ë°˜í™˜ (ì—ëŸ¬ ëŒ€ì‹ )
            print("âš ï¸ [Finalize] No analysis data found. Returning default values.")
            return {
                "total_score": 2.5,
                "result": "Incomplete",
                "stats": {"question_count": 0, "starr_counts": {}},
                "competency_scores": {"ì¢…í•©": 2.5},
                "strengths": [],  # ê°•ì  ì—†ìŒ (ì°¸ì—¬ ìì²´ëŠ” ê°•ì  ì•„ë‹˜)
                "gaps": ["ë‹µë³€ ë‚´ìš©ì´ ë¶„ì„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì¸ ê²½í—˜ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."],
                "final_feedback": "ë©´ì ‘ ë‹µë³€ì´ ì¶©ë¶„íˆ ë¶„ì„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ë©´ì ‘ì—ì„œëŠ” ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì¸ ê²½í—˜(ìƒí™©-ê³¼ì œ-í–‰ë™-ê²°ê³¼)ì„ ì„¤ëª…í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ í‰ê°€ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            }

        total_score = 0.0
        starr_counts = {"situation": 0, "task": 0, "action": 0, "result": 0, "reflection": 0}

        for analysis in analyses:
            starr = analysis.get("starr", {})
            total_score += self._score_from_starr(starr)
            for key in starr_counts:
                if starr.get(key):
                    starr_counts[key] += 1

        count = len(analyses)
        avg_score = round(total_score / count, 2)

        # ë…¼ë¬¸ ê¸°ë°˜ ìƒì„¸ ì—­ëŸ‰ ì ìˆ˜ ê³„ì‚°
        competency_scores = self._calculate_competency_scores_detailed(analyses, starr_counts)

        # ë…¼ë¬¸ ê¸°ë°˜ ê°•ì  ì‹ë³„
        strengths = self._identify_strengths_from_analyses(analyses)

        # ë…¼ë¬¸ ê¸°ë°˜ ë³´ì™„ì  ì‹ë³„
        gaps = self._identify_gaps_from_analyses(analyses, starr_counts)

        # LLMìœ¼ë¡œ ìµœì¢… í”¼ë“œë°± ìƒì„± (Conversate ë…¼ë¬¸ ê¸°ë°˜)
        final_feedback = self._generate_final_feedback(avg_score, competency_scores, strengths, gaps)

        return {
            "total_score": avg_score,
            "result": "Pass" if avg_score >= 3.0 else "Fail",
            "stats": {
                "question_count": count,
                "starr_counts": starr_counts
            },
            "competency_scores": competency_scores,
            "strengths": strengths,
            "gaps": gaps,
            "final_feedback": final_feedback,
            "history_summary": self.chat_history
        }

    def _identify_strengths_from_analyses(self, analyses: List[Dict]) -> List[str]:
        """
        ë¶„ì„ ê²°ê³¼ì—ì„œ ê°•ì  ì‹ë³„ (í˜„ì‹¤ì ì´ê³  êµ¬ì²´ì ì¸ í”¼ë“œë°±)

        ì›ì¹™:
        - "ë©´ì ‘ì— ì°¸ì—¬í–ˆë‹¤", "ë‹µë³€ì„ ì‹œë„í•¨" ê°™ì€ ë‹¹ì—°í•œ ë§ ì ˆëŒ€ ê¸ˆì§€
        - ì‹¤ì œ STARR ë¶„ì„ ê²°ê³¼ì— ê¸°ë°˜í•œ êµ¬ì²´ì  ê°•ì ë§Œ ì œì‹œ
        - ê°•ì ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì •ì§í•¨ì´ ì¤‘ìš”)
        """
        strengths = []
        evidence_clips = []

        # STARR ìš”ì†Œë³„ ì¹´ìš´íŠ¸
        situation_count = 0
        task_count = 0
        action_count = 0
        result_count = 0
        reflection_count = 0
        clear_contribution_count = 0

        total = len(analyses) if analyses else 0
        if total == 0:
            return []  # ë¶„ì„ ë°ì´í„° ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸

        for analysis in analyses:
            starr = analysis.get("starr", {})
            contribution = analysis.get("contribution", "unclear")
            clips = analysis.get("evidence_clips", [])

            if starr.get("situation"): situation_count += 1
            if starr.get("task"): task_count += 1
            if starr.get("action"): action_count += 1
            if starr.get("result"): result_count += 1
            if starr.get("reflection"): reflection_count += 1
            if contribution == "clear": clear_contribution_count += 1

            if clips:
                evidence_clips.extend(clips)

        # ë¹„ìœ¨ ê³„ì‚°
        situation_rate = situation_count / total
        task_rate = task_count / total
        action_rate = action_count / total
        result_rate = result_count / total
        reflection_rate = reflection_count / total
        contribution_rate = clear_contribution_count / total

        # ê°•ì  ì‹ë³„ (50% ì´ìƒ ë‹¬ì„± ì‹œì—ë§Œ)
        if action_rate >= 0.5 and result_rate >= 0.5:
            strengths.append("í–‰ë™(Action)ê³¼ ê²°ê³¼(Result)ë¥¼ ì—°ê²°í•˜ì—¬ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•¨")

        if situation_rate >= 0.5 and task_rate >= 0.5:
            strengths.append("ìƒí™©ê³¼ ê³¼ì œë¥¼ ëª…í™•í•˜ê²Œ ì œì‹œí•˜ì—¬ ë§¥ë½ ì´í•´ê°€ ì‰¬ì›€")

        if reflection_rate >= 0.5:
            strengths.append("ê²½í—˜ì—ì„œ ë°°ìš´ ì ì„ ì˜ í‘œí˜„í•˜ì—¬ ì„±ì¥ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤Œ")

        if contribution_rate >= 0.5:
            strengths.append("'ì €ëŠ”/ì œê°€'ë¥¼ ì‚¬ìš©í•´ ê°œì¸ ê¸°ì—¬ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•¨")

        # ì¦ê±° ê¸°ë°˜ ê°•ì  (ì‹¤ì œ ê¸°ìˆ /ìˆ˜ì¹˜ê°€ ìˆì„ ë•Œë§Œ)
        unique_evidence = list(set(evidence_clips))
        if len(unique_evidence) >= 5:
            strengths.append(f"êµ¬ì²´ì ì¸ ê¸°ìˆ ê³¼ ìˆ˜ì¹˜ë¥¼ í™œìš©í•¨ ({', '.join(unique_evidence[:3])} ë“±)")
        elif len(unique_evidence) >= 2:
            strengths.append(f"ì‹¤ì œ ì‚¬ìš©í•œ ê¸°ìˆ ì„ ì–¸ê¸‰í•¨ ({', '.join(unique_evidence[:2])})")

        # ì „ì²´ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ê²½ìš°
        all_starr_rate = (situation_rate + task_rate + action_rate + result_rate + reflection_rate) / 5
        if all_starr_rate >= 0.6:
            strengths.append("ì „ë°˜ì ìœ¼ë¡œ STARR êµ¬ì¡°ì— ë§ì¶° ì²´ê³„ì ìœ¼ë¡œ ë‹µë³€í•¨")

        # ê°•ì ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ (ì ˆëŒ€ "ì°¸ì—¬í–ˆë‹¤" ê°™ì€ ë§ ê¸ˆì§€)
        return strengths[:5]  # ìµœëŒ€ 5ê°œ

    def _identify_gaps_from_analyses(self, analyses: List[Dict], starr_counts: Dict) -> List[str]:
        """
        ë¶„ì„ ê²°ê³¼ì—ì„œ ë³´ì™„ì  ì‹ë³„ (êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í”¼ë“œë°±)

        ì›ì¹™:
        - ì¶”ìƒì ì¸ ì¡°ì–¸ì´ ì•„ë‹Œ ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„ ì 
        - "êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì¤€ë¹„í•˜ì„¸ìš”" ê°™ì€ ë»”í•œ ë§ ê¸ˆì§€
        - ì‹¤ì œ STARR ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶€ì¡±í•œ ë¶€ë¶„ë§Œ ì§€ì 
        """
        gaps = []
        total_questions = len(analyses) if analyses else 0

        if total_questions == 0:
            return ["ë‹µë³€ ë‚´ìš©ì´ ë¶„ì„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."]

        # ë¹„ìœ¨ ê³„ì‚°
        action_rate = starr_counts.get("action", 0) / total_questions
        result_rate = starr_counts.get("result", 0) / total_questions
        situation_rate = starr_counts.get("situation", 0) / total_questions
        task_rate = starr_counts.get("task", 0) / total_questions
        reflection_rate = starr_counts.get("reflection", 0) / total_questions

        # ê¸°ì—¬ë„ ë¶„ì„
        unclear_count = sum(1 for a in analyses if a.get("contribution") == "unclear")
        unclear_rate = unclear_count / total_questions

        # ì¦ê±° ë¶„ì„
        total_evidence = sum(len(a.get("evidence_clips", [])) for a in analyses)

        # ìš°ì„ ìˆœìœ„ë³„ ë³´ì™„ì  (ê°€ì¥ ë¶€ì¡±í•œ ê²ƒë¶€í„°, 30% ë¯¸ë§Œë§Œ)
        # Actionì´ ê°€ì¥ ì¤‘ìš”
        if action_rate < 0.3:
            gaps.append("'~í–ˆìŠµë‹ˆë‹¤'ë¡œ ëë‚˜ëŠ” êµ¬ì²´ì ì¸ í–‰ë™ ì„¤ëª… í•„ìš” (ì˜ˆ: 'Reactë¡œ ì»´í¬ë„ŒíŠ¸ë¥¼ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤')")

        # Resultë„ ì¤‘ìš”
        if result_rate < 0.3 and len(gaps) < 2:
            gaps.append("í–‰ë™ì˜ ê²°ê³¼ë‚˜ ì„±ê³¼ ì¶”ê°€ í•„ìš” (ì˜ˆ: 'ë°°í¬ ì™„ë£Œ', 'ì„±ëŠ¥ 30% ê°œì„ ')")

        # Situation/Task
        if situation_rate < 0.3 and task_rate < 0.3 and len(gaps) < 2:
            gaps.append("ìƒí™©/ë°°ê²½ ì„¤ëª… ì¶”ê°€ í•„ìš” (ì˜ˆ: 'ìº¡ìŠ¤í†¤ í”„ë¡œì íŠ¸ì—ì„œ ë°±ì—”ë“œë¥¼ ë‹´ë‹¹í–ˆëŠ”ë°...')")

        # ê¸°ì—¬ë„ ë¬¸ì œ
        if unclear_rate >= 0.6 and len(gaps) < 3:
            gaps.append("'ì €ëŠ”/ì œê°€'ë¡œ ì‹œì‘í•˜ì—¬ ë³¸ì¸ì´ ì§ì ‘ í•œ ì¼ì„ ëª…í™•íˆ êµ¬ë¶„í•´ ì£¼ì„¸ìš”")

        # êµ¬ì²´ì„± ë¶€ì¡± (ì¦ê±°ê°€ ì „í˜€ ì—†ì„ ë•Œë§Œ)
        if total_evidence == 0 and len(gaps) < 3:
            gaps.append("ì‚¬ìš©í•œ ê¸°ìˆ ëª…ì´ë‚˜ ìˆ˜ì¹˜(ì˜ˆ: 'Springìœ¼ë¡œ', '2ì£¼ê°„')ë¥¼ ì¶”ê°€í•˜ë©´ ì‹ ë¢°ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤")

        # ì„±ì°° ë¶€ì¡± (ë‹¤ë¥¸ ë³´ì™„ì ì´ ì—†ì„ ë•Œë§Œ)
        if reflection_rate < 0.2 and len(gaps) < 2:
            gaps.append("'ì´ ê²½í—˜ì„ í†µí•´ ~ë¥¼ ë°°ì› ìŠµë‹ˆë‹¤'ë¡œ ë§ˆë¬´ë¦¬í•˜ë©´ ë‹µë³€ì´ ë” ì™„ì„±ë©ë‹ˆë‹¤")

        # âœ… [FIX] ë³´ì™„ì ì´ ì—†ì§€ë§Œ ì ìˆ˜ê°€ ë‚®ì„ ë•Œ (ë¶ˆí•©ê²© ì˜ˆìƒ) - ê°€ì¥ ë¶€ì¡±í•œ ìš”ì†Œ ì§€ì 
        if len(gaps) == 0:
            # ê°€ì¥ ë¶€ì¡±í•œ ìš”ì†Œ ì°¾ê¸°
            rates = [
                ("í–‰ë™(Action)", action_rate, "'~í–ˆìŠµë‹ˆë‹¤'ë¡œ ëë‚˜ëŠ” êµ¬ì²´ì ì¸ í–‰ë™ ì„¤ëª…ì„ ë” ì¶”ê°€í•´ ë³´ì„¸ìš”"),
                ("ê²°ê³¼(Result)", result_rate, "í–‰ë™ì˜ ê²°ê³¼ë‚˜ ì„±ê³¼(ìˆ˜ì¹˜, ë³€í™”)ë¥¼ í¬í•¨í•˜ë©´ ë” ì„¤ë“ë ¥ ìˆìŠµë‹ˆë‹¤"),
                ("ìƒí™©(Situation)", situation_rate, "ìƒí™©/ë°°ê²½ ì„¤ëª…ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì‹œë©´ ì¢‹ê² ìŠµë‹ˆë‹¤"),
                ("ì„±ì°°(Reflection)", reflection_rate, "ê²½í—˜ì—ì„œ ë°°ìš´ ì ì´ë‚˜ ëŠë‚€ ì ì„ ì¶”ê°€í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤"),
            ]
            # ê°€ì¥ ë‚®ì€ ë¹„ìœ¨ ìš”ì†Œ ì°¾ê¸°
            min_rate_item = min(rates, key=lambda x: x[1])
            if min_rate_item[1] < 0.7:  # 70% ë¯¸ë§Œì´ë©´ ë³´ì™„ì  ì¶”ê°€
                gaps.append(min_rate_item[2])

        return gaps[:3]  # ìµœëŒ€ 3ê°œ

    def _calculate_competency_scores_detailed(self, analyses: List[Dict], starr_counts: Dict) -> Dict[str, float]:
        """
        ìƒì„¸ ì—­ëŸ‰ ì ìˆ˜ ê³„ì‚° (ë…¼ë¬¸ ê¸°ë°˜)

        ì—­ëŸ‰ êµ¬ë¶„:
        1. ê¸°ìˆ  ì—­ëŸ‰: ë‚´ìš© ì •í™•ì„±, ê¹Šì´, êµ¬ì¡°
        2. ì†Œí”„íŠ¸ ìŠ¤í‚¬: ëª…í™•ì„±, ìì‹ ê°, ì¼ê´€ì„±
        3. STARR êµ¬ì¡°: ê° ìš”ì†Œë³„ ë‹¬ì„±ë„
        """
        total = len(analyses) if analyses else 1

        # 1. STARR ê¸°ë°˜ ì ìˆ˜ (ê° ìš”ì†Œë³„)
        starr_scores = {
            "ìƒí™©_ì¸ì‹": round((starr_counts.get("situation", 0) / total) * 5, 1),
            "ê³¼ì œ_ëª…í™•ì„±": round((starr_counts.get("task", 0) / total) * 5, 1),
            "í–‰ë™_êµ¬ì²´ì„±": round((starr_counts.get("action", 0) / total) * 5, 1),
            "ê²°ê³¼_ì§€í–¥ì„±": round((starr_counts.get("result", 0) / total) * 5, 1),
            "ì„±ì°°_ê¹Šì´": round((starr_counts.get("reflection", 0) / total) * 5, 1),
        }

        # 2. ê¸°ì—¬ë„ ì ìˆ˜
        contribution_score = 0
        for analysis in analyses:
            contrib = analysis.get("contribution", "unclear")
            if contrib == "clear":
                contribution_score += 5
            elif contrib == "mixed":
                contribution_score += 3
            else:
                contribution_score += 1
        contribution_avg = round(contribution_score / total, 1) if total > 0 else 0

        # 3. ì¢…í•© ì ìˆ˜
        starr_avg = sum(starr_scores.values()) / len(starr_scores) if starr_scores else 0
        overall = round((starr_avg * 0.7 + contribution_avg * 0.3), 1)

        return {
            **starr_scores,
            "ê¸°ì—¬ë„_ëª…í™•ì„±": contribution_avg,
            "ì¢…í•©": overall
        }

    def _generate_final_feedback(self, score: float, competency_scores: Dict, strengths: List[str], gaps: List[str]) -> str:
        """
        LLMì„ ì‚¬ìš©í•´ ìµœì¢… í”¼ë“œë°± ìƒì„± (Conversate ë…¼ë¬¸ ê¸°ë°˜)
        - ì‹¤ì œ ë©´ì ‘ê´€ì²˜ëŸ¼ ìì—°ìŠ¤ëŸ¬ìš´ í”¼ë“œë°±
        - êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆ ì œì‹œ
        - ë¹ˆë§ì´ë‚˜ í˜•ì‹ì  ì¹­ì°¬ ê¸ˆì§€
        """
        if not self.client:
            # LLM ì—†ì„ ë•Œ ê¸°ë³¸ í”¼ë“œë°± (ë” í˜„ì‹¤ì ìœ¼ë¡œ)
            if score >= 4.0:
                return "ë‹µë³€ì—ì„œ êµ¬ì²´ì ì¸ ê²½í—˜ê³¼ ì„±ê³¼ë¥¼ ì˜ ì„¤ëª…í•´ì£¼ì…¨ìŠµë‹ˆë‹¤. íŠ¹íˆ ë³¸ì¸ì´ ì§ì ‘ í•œ ì¼ê³¼ ê·¸ ê²°ê³¼ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•œ ì ì´ ì¢‹ì•˜ìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œë„ ì´ëŸ° ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì‹œë©´ ë©ë‹ˆë‹¤."
            elif score >= 3.0:
                return "ì „ë°˜ì ìœ¼ë¡œ ê´œì°®ì€ ë‹µë³€ì´ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ì¼ë¶€ ë‹µë³€ì—ì„œ 'êµ¬ì²´ì ìœ¼ë¡œ ë¬´ì—‡ì„ í–ˆëŠ”ì§€'ê°€ ë” ëª…í™•í–ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. í–‰ë™ê³¼ ê²°ê³¼ë¥¼ ìˆ˜ì¹˜ë‚˜ ì‚¬ë¡€ë¡œ í‘œí˜„í•˜ë©´ ë” ì„¤ë“ë ¥ ìˆëŠ” ë‹µë³€ì´ ë©ë‹ˆë‹¤."
            else:
                return "ë‹µë³€ ì‹œ ìƒí™© ì„¤ëª…ì€ ì¢‹ì•˜ì§€ë§Œ, ë³¸ì¸ì´ ì‹¤ì œë¡œ í•œ í–‰ë™ê³¼ ê·¸ ê²°ê³¼ê°€ ë” êµ¬ì²´ì ì´ì—ˆìœ¼ë©´ í•©ë‹ˆë‹¤. ë‹¤ìŒ ë©´ì ‘ì—ì„œëŠ” 'ì €ëŠ” ~ì„ í•´ì„œ ~í•œ ê²°ê³¼ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤' í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ ë³´ì„¸ìš”."

        try:
            # ê°•ì ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ (í˜•ì‹ì  ì¹­ì°¬ ë°©ì§€)
            strengths_text = "\n".join([f"- {s}" for s in strengths]) if strengths else "(íŠ¹ë³„í•œ ê°•ì  ì—†ìŒ)"
            gaps_text = "\n".join([f"- {g}" for g in gaps]) if gaps else "(íŠ¹ë³„í•œ ë³´ì™„ì  ì—†ìŒ)"

            prompt = f"""ë‹¹ì‹ ì€ ì‹¤ì œ ê¸°ì—…ì˜ ê¸°ìˆ  ë©´ì ‘ê´€ì…ë‹ˆë‹¤. ë©´ì ‘ ê²°ê³¼ì— ëŒ€í•´ ì†”ì§í•˜ê³  ì‹¤ìš©ì ì¸ í”¼ë“œë°±ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

## ë©´ì ‘ ê²°ê³¼
- ì¢…í•© ì ìˆ˜: {score}/5.0
- ì—­ëŸ‰ë³„ ì ìˆ˜: {json.dumps(competency_scores, ensure_ascii=False)}

## ë¶„ì„ëœ ê°•ì 
{strengths_text}

## ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­
{gaps_text}

## í”¼ë“œë°± ì‘ì„± ì›ì¹™
1. ë¹ˆë§ì´ë‚˜ í˜•ì‹ì ì¸ ì¹­ì°¬ ê¸ˆì§€ (ì˜ˆ: "ë©´ì ‘ì— ì°¸ì—¬í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤" ê°™ì€ ë§ ê¸ˆì§€)
2. ì‹¤ì œë¡œ ì˜í•œ ë¶€ë¶„ì´ ìˆìœ¼ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰
3. ê°œì„ ì ì€ ë°”ë¡œ ì‹¤ì²œí•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ íŒìœ¼ë¡œ ì œì‹œ
4. ë©´ì ‘ê´€ì´ ì‹¤ì œë¡œ í•  ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì–´íˆ¬ ì‚¬ìš©
5. 2-3ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ë§Œ ì „ë‹¬

## ì ìˆ˜ë³„ í”¼ë“œë°± ë°©í–¥
- 4.0 ì´ìƒ: ì˜í•œ ì  ìœ„ì£¼ë¡œ, ì‘ì€ ê°œì„ ì  í•˜ë‚˜
- 3.0-3.9: ì˜í•œ ì ê³¼ ê°œì„ ì  ê· í˜•ìˆê²Œ
- 3.0 ë¯¸ë§Œ: ê°œì„ ì  ìœ„ì£¼ë¡œ, êµ¬ì²´ì ì¸ ì—°ìŠµ ë°©ë²• ì œì•ˆ

í•œêµ­ì–´ë¡œ í”¼ë“œë°±ì„ ì‘ì„±í•´ì£¼ì„¸ìš” (JSON í˜•ì‹ ì•„ë‹˜, ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ë§Œ):"""

            response = self._call_llm(prompt)
            if response:
                # JSONì´ë‚˜ ë¶ˆí•„ìš”í•œ í˜•ì‹ ì œê±°
                cleaned = response.strip()
                if cleaned.startswith('"') and cleaned.endswith('"'):
                    cleaned = cleaned[1:-1]
                return cleaned
            return "ë‹µë³€ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°œì„ ì ì„ íŒŒì•…í•˜ì‹œê³ , ë‹¤ìŒ ë©´ì ‘ì—ì„œëŠ” êµ¬ì²´ì ì¸ ê²½í—˜ê³¼ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•´ ë‹µë³€í•´ ë³´ì„¸ìš”."
        except Exception as e:
            print(f"âš ï¸ Failed to generate final feedback: {e}")
            return "ë‹µë³€ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°œì„ ì ì„ íŒŒì•…í•˜ì‹œê³ , ë‹¤ìŒ ë©´ì ‘ì—ì„œëŠ” êµ¬ì²´ì ì¸ ê²½í—˜ê³¼ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•´ ë‹µë³€í•´ ë³´ì„¸ìš”."
