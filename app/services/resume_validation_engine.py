import json
import re
import os
import pickle
import numpy as np
from typing import List, Dict, Optional
from pathlib import Path

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    from openai import OpenAI
except ImportError as e:
    print(f"âš ï¸ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë¡œê·¸ë¥¼ ë‚¨ê¸°ê±°ë‚˜ ì—ëŸ¬ë¥¼ raise í•  ìˆ˜ ìˆìŒ

# ==========================================
# 1. ë°ì´í„° ë¡œë“œ ë° ê²½ë¡œ ì„¤ì •
# ==========================================
def get_project_root() -> Path:
    """
    í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    app/services/resume_validation_engine.py -> í”„ë¡œì íŠ¸ ë£¨íŠ¸
    """
    current_file = Path(__file__).resolve()
    return current_file.parent.parent.parent

def get_data_path() -> Path:
    """
    ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    return get_project_root() / "app" / "data"

class DataLoader:
    """
    ê¸°ì¡´ ë°ì´í„° ë¡œë” ìœ ì§€ (í•„ìš” ì‹œ ë‹¤ë¥¸ ë©”íƒ€ë°ì´í„° ì ‘ê·¼ìš©)
    """
    def __init__(self):
        self.base_path = get_data_path()
        self.file_names = {
            "resumes": "final_resume_600.json",
            "companies": "company_50_pool.json",
            "metadata": "final_metadata_600.json"
        }
        self.data = {}
        # í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì„œ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ìˆìŒ. 
        # í˜„ì¬ MatchingEngineì€ pickle íŒŒì¼ì„ ì§ì ‘ ë¡œë“œí•˜ë¯€ë¡œ,
        # ì—¬ê¸°ì„œëŠ” ê²½ë¡œ í™•ì¸ ì •ë„ë§Œ ìˆ˜í–‰í•˜ê±°ë‚˜ ë¹„ì›Œë‘˜ ìˆ˜ ìˆìŒ.
        
    def normalize(self, text):
        if not text: return ""
        return re.sub(r'[^a-zA-Z0-9]', '', str(text).lower())

# ==========================================
# 2. ë§¤ì¹­ ì—”ì§„ (Hybrid Vector + Keyword)
# ==========================================
class MatchingEngine:
    """
    [Core Engine]
    í•˜ì´ë¸Œë¦¬ë“œ ë§¤ì¹­ (ë²¡í„° 55% + í‚¤ì›Œë“œ 35%) + [Metadata Bonus 10%]
    + [Smart Calibration] (ëœë¤ì´ ì•„ë‹Œ, ì‹¤ë ¥ ê¸°ë°˜ ì ìˆ˜ ë§¤í•‘)
    """

    # 1. ë§¤ì¹­ ê°€ì¤‘ì¹˜ (Total 1.0)
    WEIGHT_VECTOR = 0.55
    WEIGHT_KEYWORD = 0.35
    BONUS_ROLE_MATCH = 0.10

    # 2. ë“±ê¸‰ë³„ ì¶”ì²œ ê¸°ì—… í‹°ì–´ (Quota)
    TIER_RULES = {
        "S": ["Top", "Top", "Mid"],
        "A": ["Top", "Mid", "Mid"],
        "B": ["Mid", "Mid", "Low"],
        "C": ["Mid", "Low", "Low"],
        "F": ["Low", "Low", "Low"]
    }

    # 3. ëª©í‘œ ì ìˆ˜ êµ¬ê°„ (ì‚¬ìš©ì ë§Œì¡±ìš©)
    SCORE_RANGES = [
        (88.0, 97.0), # Rank 1
        (77.0, 86.0), # Rank 2
        (66.0, 75.0)  # Rank 3
    ]

    # [NEW] í˜„ì‹¤ì ì¸ Raw Score ê¸°ì¤€ì  (ì •ê·œí™” í›„ ê¸°ì¤€)
    RAW_SCORE_MIN = 0.30
    RAW_SCORE_MAX = 0.95
    GAP_THRESHOLD = 0.50  # 0.5ì (50ì ) ë¯¸ë§Œì´ë©´ ê²½ê³ 

    def __init__(self):
        print("ğŸš€ ë§¤ì¹­ ì—”ì§„(Matching Engine) ì´ˆê¸°í™” ì¤‘...")
        
        self.base_path = get_data_path()
        self.model_name = "jhgan/ko-sroberta-multitask"
        
        # OpenAI Client (í™˜ê²½ë³€ìˆ˜ì—ì„œ í‚¤ ë¡œë“œ)
        api_key = os.environ.get("OPENAI_API_KEY")
        self.client = OpenAI(api_key=api_key) if api_key else None

        # Model Load
        print(f"   -> ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}")
        # ëª¨ë¸ ë¡œë”©ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ì‹±ê¸€í†¤ íŒ¨í„´ì´ë‚˜ ì‹œì‘ ì‹œ ë¡œë“œë¥¼ ê³ ë ¤í•´ì•¼ í•¨
        # ì—¬ê¸°ì„œëŠ” ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œ ë¡œë“œ
        self.model = SentenceTransformer(self.model_name)

        # Data Load
        self.company_data = self._load_company_vectors()
        
        # DataLoader ì¸ìŠ¤í„´ìŠ¤ (ë³´ì¡°ìš©)
        self.dl = DataLoader()

    def _load_company_vectors(self):
        """pkl íŒŒì¼ ë¡œë“œ"""
        pkl_path = self.base_path / "company_jd_vectors.pkl"
        
        if not pkl_path.exists():
            print(f"âŒ ê¸°ì—… ë²¡í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pkl_path}")
            return None
            
        try:
            with open(pkl_path, 'rb') as f:
                data = pickle.load(f)
            if isinstance(data, dict) and 'vectors' in data:
                print(f"   -> ê¸°ì—… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data['companies'])}ê°œ ê¸°ì—…")
                return data
            else:
                print("âŒ pkl íŒŒì¼ êµ¬ì¡° ì˜¤ë¥˜")
                return None
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}")
            return None

    def _calculate_keyword_score(self, resume_text: str, tech_stack: List[str]) -> float:
        if not tech_stack: return 0.5
        resume_lower = resume_text.lower()
        match_count = sum(1 for t in tech_stack if t.lower() in resume_lower)
        return match_count / len(tech_stack)

    def _calculate_metadata_bonus(self, candidate_role: str, company_target_roles: List[str]) -> float:
        if not candidate_role or not company_target_roles: return 0.0
        cand_role_lower = candidate_role.lower()
        for role in company_target_roles:
            role_lower = role.lower()
            if role_lower in cand_role_lower or cand_role_lower in role_lower:
                return self.BONUS_ROLE_MATCH
            if "fullstack" in cand_role_lower and role_lower in ["backend", "frontend"]:
                return self.BONUS_ROLE_MATCH * 0.8
            if "ai" in cand_role_lower and role_lower in ["nlp", "llm", "vision"]:
                return self.BONUS_ROLE_MATCH
        return 0.0

    def _normalize_vector_score(self, val: float) -> float:
        """
        [New] S-BERT Cosine Similarity ì •ê·œí™”
        ê¸°ê³„ì  ìœ ì‚¬ë„(0.15~0.75)ë¥¼ ì¸ê°„ì´ ì´í•´í•˜ëŠ” ì ìˆ˜(0.0~1.0)ë¡œ ë³€í™˜
        """
        min_bound = 0.15
        max_bound = 0.75

        normalized = (val - min_bound) / (max_bound - min_bound)
        return max(0.0, min(1.0, normalized))

    def _map_score_to_range(self, raw_score: float, target_min: float, target_max: float) -> float:
        """
        [Dynamic Scaling] í˜„ì‹¤ì ì¸ ì…ë ¥ ë²”ìœ„(Raw Score)ë¥¼ ëª©í‘œ ë²”ìœ„ë¡œ ë§¤í•‘
        """
        input_min, input_max = self.RAW_SCORE_MIN, self.RAW_SCORE_MAX

        normalized = (raw_score - input_min) / (input_max - input_min)
        normalized = max(0.0, min(1.0, normalized))

        scaled_score = target_min + (normalized * (target_max - target_min))
        return round(scaled_score, 1)

    def _categorize_companies(self, all_companies, vector_scores, resume_text, candidate_role):
        buckets = {"Top": [], "Mid": [], "Low": []}

        for idx, comp in enumerate(all_companies):
            # 1. ë²¡í„° ì ìˆ˜ ì •ê·œí™”
            v_raw = float(vector_scores[idx])
            v_norm = self._normalize_vector_score(v_raw)

            # 2. í‚¤ì›Œë“œ ì ìˆ˜
            k_score = self._calculate_keyword_score(resume_text, comp.get('tech_stack', []))

            # Semantic Rescue (í‚¤ì›Œë“œ 0ì  êµ¬ì œ)
            if k_score == 0.0 and v_norm > 0.5: k_score = 0.2

            # 3. ê°€ì¤‘ì¹˜ ì ìš© í•©ì‚°
            hybrid_score = (v_norm * self.WEIGHT_VECTOR) + (k_score * self.WEIGHT_KEYWORD)

            # 4. ë©”íƒ€ë°ì´í„° ë³´ë„ˆìŠ¤
            meta_bonus = self._calculate_metadata_bonus(candidate_role, comp.get('target_roles', []))

            final_raw_score = hybrid_score + meta_bonus
            final_raw_score = min(1.0, final_raw_score)

            comp_data = {
                "metadata": {
                    "company_name": comp["name"],
                    "job_title": ", ".join(comp.get("target_roles", [])),
                    "industry": comp["industry"],
                    "tier": comp.get("tier", "Low")
                },
                "tech_stack": comp.get("tech_stack", []), # ë‚´ë¶€ ë¡œì§ìš©
                "raw_score": final_raw_score,
                "vector_raw": round(v_raw, 2),
                "vector_norm": round(v_norm, 2),
                "keyword_raw": round(k_score, 2),
                "meta_bonus": round(meta_bonus, 2)
            }
            # MatchResult ëª¨ë¸ í˜¸í™˜ì„±ì„ ìœ„í•´ flat í•˜ê²Œ ì €ì¥í•˜ì§€ ì•Šê³  metadata êµ¬ì¡° ìœ ì§€í•˜ë˜,
            # ë‚´ë¶€ ë¡œì§ì—ì„œëŠ” comp_data ì ‘ê·¼
            
            # API í˜¸í™˜ì„ ìœ„í•´ company_name ë“±ì€ metadata ì•ˆì— ë„£ê³ , 
            # ì¶”ì²œ ë¡œì§ ë‚´ì—ì„œëŠ” í¸ì˜ìƒ í‚¤ ì ‘ê·¼

            tier = comp.get("tier", "Low")
            if tier not in buckets: tier = "Low"
            buckets[tier].append(comp_data)

        for t in buckets:
            buckets[t].sort(key=lambda x: x['raw_score'], reverse=True)

        return buckets

    def _convert_resume_to_text(self, resume_input: dict) -> str:
        """
        ì´ë ¥ì„œ JSON ê°ì²´ë¥¼ ì„ë² ë”© ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        """
        parts = []
        
        # 1. ìŠ¤í‚¬
        content = resume_input.get('resume_content', {})
        skills = content.get('skills', {})
        essential = skills.get('essential', [])
        additional = skills.get('additional', [])
        all_skills = essential + additional
        if all_skills:
            parts.append(f"Technical Skills: {', '.join(all_skills)}")
            
        # 2. ê²½ë ¥ (Key Tasks ìœ„ì£¼)
        experiences = content.get('professional_experience', [])
        for exp in experiences:
            role = exp.get('role', '')
            tasks = exp.get('key_tasks', [])
            parts.append(f"Role: {role}")
            if tasks:
                parts.append(f"Tasks: {', '.join(tasks)}")
                
        # 3. í”„ë¡œì íŠ¸
        projects = content.get('project_experience', [])
        for proj in projects:
            title = proj.get('project_title', '')
            achievements = proj.get('key_achievements', [])
            parts.append(f"Project: {title}")
            if achievements:
                parts.append(f"Achievements: {', '.join(achievements)}")
                
        # 4. ë¶„ì„ëœ ì§ë¬´ (Target Role)
        classification = resume_input.get('classification', {})
        role = classification.get('predicted_role', '')
        if not role:
             role = resume_input.get('target_role', '')
        if role:
            parts.append(f"Target Role: {role}")
            
        return "\n".join(parts)

    def generate_xai_feedback(self, resume_input: dict, recommendations: List[Dict]) -> str:
        """
        [ê¸°ëŠ¥ ìˆ˜ì •] ì „ì²´ ì¶”ì²œ ê¸°ì—… ëª©ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ
        ì¹œê·¼í•˜ê³  êµ¬ì²´ì ì¸ 'AI ì½”ì¹˜' ìŠ¤íƒ€ì¼ì˜ ì¢…í•© í”¼ë“œë°± ë¬¸ì¥ ìƒì„±
        """
        feedback_lines = ["\nì¢…í•© AI ì½”ì¹˜ ì˜ê²¬:"]

        if not recommendations:
            feedback_lines.append("ì œê³µëœ ì´ë ¥ì„œì— ë§ëŠ” ì¶”ì²œ ê¸°ì—…ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì´ë ¥ì„œ ë‚´ìš©ì„ ì ê²€í•´ì£¼ì„¸ìš”.")
            return "\n".join(feedback_lines)

        # 1. ìµœê³  ë§¤ì¹­ ê¸°ì—… ì •ë³´ ë¶„ì„
        top_rec = recommendations[0]
        # comp_data êµ¬ì¡°ê°€ _categorize_companiesì—ì„œ ì •ì˜ë¨
        # metadata ë‚´ë¶€ì— company_nameì´ ìˆìŒ
        top_company_name = top_rec['metadata']['company_name']
        top_score = top_rec['match_score'] # recommend_companiesì—ì„œ ê³„ì‚°ë˜ì–´ ì¶”ê°€ë¨
        top_note = top_rec.get('note', '')

        # ì´ë ¥ì„œì˜ í•µì‹¬ ìŠ¤í‚¬ ì¶”ì¶œ
        content = resume_input.get('resume_content', {})
        skills = content.get('skills', {})
        resume_essential_skills = set(skills.get('essential', []))
        resume_additional_skills = set(skills.get('additional', []))
        resume_all_skills = resume_essential_skills.union(resume_additional_skills)

        exp_tasks_summary = []
        for exp in content.get('professional_experience', [])[:1]: # Top 1 experience
            exp_tasks_summary.extend(exp.get('key_tasks', [])[:2]) # Top 2 tasks
        exp_summary_str = ", ".join(exp_tasks_summary) if exp_tasks_summary else "ë‹¤ì–‘í•œ í”„ë¡œì íŠ¸ ê²½í—˜"

        # 2. ì¢…í•© í‰ê°€ ë©˜íŠ¸
        feedback_lines.append(f"ì´ë ¥ì„œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ ë³´ë‹ˆ, ìµœê³  ë§¤ì¹­ ê¸°ì—…ì¸ **{top_company_name}**ì—ì„œ {top_score}ì ìœ¼ë¡œ '{top_note}' í‰ê°€ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.")

        if top_score >= 88:
            skills_str = ', '.join(list(resume_all_skills)[:3])
            feedback_lines.append(f"ì§€ì›ìë‹˜ì˜ **{skills_str}** ë“±ì˜ í•µì‹¬ ì—­ëŸ‰ê³¼ **{exp_summary_str}** ê²½í—˜ì´ í•´ë‹¹ ê¸°ì—…ì˜ ìš”êµ¬ì‚¬í•­ê³¼ ë§¤ìš° ì˜ ë§ì•„ ë–¨ì–´ì§‘ë‹ˆë‹¤. ì´ ê°•ì ì„ ì ê·¹ì ìœ¼ë¡œ ì–´í•„í•˜ë©´ ì¢‹ì€ ê²°ê³¼ê°€ ìˆì„ ê²ƒì…ë‹ˆë‹¤! ğŸš€")
        elif top_score >= 76:
            skills_str = ', '.join(list(resume_all_skills)[:2])
            feedback_lines.append(f"ì „ë°˜ì ìœ¼ë¡œ ì•ˆì •ì ì¸ ê¸°ìˆ  í•ì„ ë³´ì—¬ì£¼ë©°, íŠ¹íˆ **{skills_str}** ì—­ëŸ‰ì€ ì¶©ë¶„í•©ë‹ˆë‹¤. ë©´ì ‘ì—ì„œ **{exp_summary_str}** ê²½í—˜ê³¼ ì„±ì¥ ê°€ëŠ¥ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ ì „ë‹¬í•œë‹¤ë©´ í•©ê²©ê¶Œì— ë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ’ª")
        else:
            # ì „ì²´ ì¶”ì²œ ëª©ë¡ì—ì„œ 'Skill Gap'ì´ ìˆëŠ” íšŒì‚¬ë“¤ì„ ì°¾ì•„ ë¶€ì¡±í•œ ìŠ¤í‚¬ì…‹ì„ ì–¸ê¸‰
            all_missing_skills = set()
            for rec in recommendations:
                comp_stack = set(rec.get('tech_stack', []))
                missing = comp_stack - resume_all_skills
                if missing: all_missing_skills.update(list(missing)[:1])
            
            missing_str = ", ".join(list(all_missing_skills)[:3]) if all_missing_skills else "íŠ¹ì • ê¸°ìˆ  ìŠ¤íƒ"

            feedback_lines.append(f"ì•„ì‰½ê²Œë„ ì¶”ì²œëœ ê¸°ì—…ë“¤, íŠ¹íˆ **{top_company_name}**ì—ì„œëŠ” **{missing_str}** ê´€ë ¨ ì—­ëŸ‰ì— ëŒ€í•œ ë³´ì™„ì´ í•„ìš”í•˜ë‹¤ëŠ” ì˜ê²¬ì´ ìˆì—ˆìŠµë‹ˆë‹¤.")
            feedback_lines.append("ì´ë ¥ì„œì—ì„œ ì–¸ê¸‰ëœ ë¶€ì¡± ìŠ¤í‚¬ì— ëŒ€í•œ í•™ìŠµ ê³„íšì´ë‚˜ ê´€ë ¨ í”„ë¡œì íŠ¸ ê²½í—˜ì„ ê°•ì¡°í•˜ì—¬ ì„±ì¥ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. í¬ê¸°í•˜ì§€ ì•Šê³  ê¾¸ì¤€íˆ ë°œì „í•˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ì£¼ì„¸ìš”! ğŸŒŸ")

        return "\n".join(feedback_lines)

    def recommend(self, resume_input: dict):
        """
        FastAPI ë¼ìš°í„° í˜¸í™˜ìš© ë©”ì¸ ë©”ì†Œë“œ
        """
        # [ë°©ì–´ ì½”ë“œ] ê¸°ì—… ë°ì´í„° í™•ì¸
        if not self.company_data:
            return [], "ì‹œìŠ¤í…œ ì—ëŸ¬: ê¸°ì—… ë°ì´í„°(Vector DB)ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        # 1. ì´ë ¥ì„œ í…ìŠ¤íŠ¸ ë³€í™˜
        resume_text = self._convert_resume_to_text(resume_input)
        
        # 2. ì§ë¬´ íŒŒì•…
        classification = resume_input.get('classification', {})
        role = classification.get('predicted_role', '')
        if not role:
             role = resume_input.get('target_role', 'backend') # default

        # 3. ë²¡í„° ì„ë² ë”© ë° ìœ ì‚¬ë„ ê³„ì‚°
        query_vector = self.model.encode([resume_text])
        all_vectors = self.company_data['vectors']
        vector_scores = cosine_similarity(query_vector, all_vectors)[0]

        # 4. ë²„í‚·íŒ… ë° ì ìˆ˜ ê³„ì‚°
        buckets = self._categorize_companies(self.company_data['companies'], vector_scores, resume_text, role)

        # 5. ë“±ê¸‰ ê¸°ë°˜ ê¸°ì—… ì„ ì • (candidate_gradeëŠ” DTOì— ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ Bë¡œ ê°€ì •í•˜ê±°ë‚˜ ì ìˆ˜ ê¸°ë°˜ ì—­ì‚° ê°€ëŠ¥í•˜ë‚˜, ì—¬ê¸°ì„  B default)
        # resume_inputì— grade ì •ë³´ê°€ ìˆë‹¤ë©´ ì‚¬ìš©
        # resume_evaluation = resume_input.get('resume_evaluation', {})
        # candidate_grade = resume_evaluation.get('grade', 'B') if resume_evaluation else 'B'
        candidate_grade = "B" # ê¸°ë³¸ê°’

        target_slots = self.TIER_RULES.get(candidate_grade, self.TIER_RULES["B"])
        final_selection = []
        used_companies = set()

        for required_tier in target_slots:
            selected = None
            for comp in buckets.get(required_tier, []):
                comp_name = comp['metadata']['company_name']
                if comp_name not in used_companies:
                    selected = comp
                    break

            if not selected:
                search_order = ["Top", "Mid", "Low"] if required_tier == "Top" else ["Mid", "Low", "Top"]
                for tier in search_order:
                    for comp in buckets.get(tier, []):
                        comp_name = comp['metadata']['company_name']
                        if comp_name not in used_companies:
                            selected = comp
                            break
                    if selected: break

            if selected:
                used_companies.add(selected['metadata']["company_name"])
                final_selection.append(selected)

        # 6. ì ìˆ˜ ë§¤í•‘ (Smart Calibration) ë° ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = []
        for i, res in enumerate(final_selection):
            # Dynamic Scaling
            if i < len(self.SCORE_RANGES):
                min_s, max_s = self.SCORE_RANGES[i]
                final_score = self._map_score_to_range(res['raw_score'], min_s, max_s)
            else:
                final_score = round(res['raw_score'] * 100, 1)

            # Note ì„¤ì •
            if i == 0:
                note = "ğŸ† Best Match"
            elif res['raw_score'] < self.GAP_THRESHOLD:
                note = "âš ï¸ Skill Gap"
            else:
                note = "âœ… High Fit"

            # ë‚´ë¶€ ë”•ì…”ë„ˆë¦¬ ì—…ë°ì´íŠ¸ (feedback ìƒì„±ìš©)
            res['match_score'] = final_score
            res['note'] = note
            
            # API ë°˜í™˜ìš© êµ¬ì¡°ë¡œ ë³€í™˜
            # MatchResult: company_name, match_score, tier, match_type, reason
            # api/routes.py í˜¸í™˜ì„ ìœ„í•´ raw_score, is_exact_match ì¶”ê°€
            formatted_results.append({
                "metadata": res['metadata'], # ê¸°ì¡´ êµ¬ì¡° í˜¸í™˜
                "company_name": res['metadata']['company_name'], # API í•„ë“œ
                "match_score": final_score,
                "tier": res['metadata']['tier'],
                "match_type": note,
                "reason": f"Tech Match: {res['keyword_raw']*100:.0f}%, Vector: {res['vector_norm']:.2f}",
                
                # [Legacy Support] api/routes.py í˜¸í™˜
                "raw_score": final_score, 
                "is_exact_match": (note == "ğŸ† Best Match") or (final_score >= 85),
                
                # ë‚´ë¶€ ë¡œì§ìš© í•„ë“œ ìœ ì§€ (feedback ìš©)
                "tech_stack": res['tech_stack'],
                "note": note
            })

        # 7. í”¼ë“œë°± ìƒì„±
        report = self.generate_xai_feedback(resume_input, formatted_results)

        return formatted_results, report

# Singleton Instance
resume_validation_engine = MatchingEngine()
